{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tfidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "vect = joblib.load(\"ruentfidf/tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"final_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"name\": [],\n",
    "    \"train\": [],\n",
    "    \"val\": [],\n",
    "    \"test\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"COPA/train.jsonl\"\n",
    "val_path = data_dir / \"COPA/val.jsonl\"\n",
    "test_path = data_dir / \"COPA/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_copa(row):\n",
    "    premise = str(row[\"premise\"]).strip()\n",
    "    choice1 = row[\"choice1\"]\n",
    "    choice2 = row[\"choice2\"]\n",
    "    label = row[\"label\"]\n",
    "    question = \"Что было ПРИЧИНОЙ этого?\" if row[\"question\"] == \"cause\" else \"Что случилось в РЕЗУЛЬТАТЕ?\"\n",
    "    res = f\"{premise} {question} {choice1} {choice2}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_copa(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_copa, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_copa(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_copa(train_path, val_path, test_path):\n",
    "    train = build_features_copa(train_path)\n",
    "    val = build_features_copa(val_path)\n",
    "    test = build_features_copa(test_path)\n",
    "    clf = fit_copa(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf, copa_scores = eval_copa(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.775, 'val': 0.45, 'test': 0.486}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"COPA\")\n",
    "all_results[\"train\"].append(copa_scores[\"train\"])\n",
    "all_results[\"val\"].append(copa_scores[\"val\"])\n",
    "all_results[\"test\"].append(copa_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommitmentBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"CommitmentBank/train.jsonl\"\n",
    "val_path = data_dir / \"CommitmentBank/val.jsonl\"\n",
    "test_path = data_dir / \"CommitmentBank/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_commitment_bank(row):\n",
    "    premise = str(row[\"premise\"]).strip()\n",
    "    hypothesis = row[\"hypothesis\"]\n",
    "    label = row[\"label\"]\n",
    "    res = f\"{premise} {hypothesis}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_commitment_bank(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_commitment_bank, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_commitment_bank(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_commitment_bank(train_path, val_path, test_path):\n",
    "    train = build_features_commitment_bank(train_path)\n",
    "    val = build_features_commitment_bank(val_path)\n",
    "    test = build_features_commitment_bank(test_path)\n",
    "    clf = fit_commitment_bank(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf, commitment_bank_scores = eval_commitment_bank(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.7420091324200914,\n",
       " 'val': 0.5227272727272727,\n",
       " 'test': 0.4520547945205479}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commitment_bank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"CommitmentBank\")\n",
    "all_results[\"train\"].append(commitment_bank_scores[\"train\"])\n",
    "all_results[\"val\"].append(commitment_bank_scores[\"val\"])\n",
    "all_results[\"test\"].append(commitment_bank_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"BoolQ/train.jsonl\"\n",
    "val_path = data_dir / \"BoolQ/val.jsonl\"\n",
    "test_path = data_dir / \"BoolQ/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_bool_q(row):\n",
    "    res = str(row[\"question\"]).strip()\n",
    "    label = row[\"label\"]\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_bool_q(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_bool_q, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_bool_q(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_bool_q(train_path, val_path, test_path):\n",
    "    train = build_features_bool_q(train_path)\n",
    "    val = build_features_bool_q(val_path)\n",
    "    test = build_features_bool_q(test_path)\n",
    "    clf = fit_copa(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, bool_q_scores = eval_bool_q(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.7321428571428571,\n",
       " 'val': 0.6644067796610169,\n",
       " 'test': 0.6847457627118644}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_q_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"BoolQ\")\n",
    "all_results[\"train\"].append(bool_q_scores[\"train\"])\n",
    "all_results[\"val\"].append(bool_q_scores[\"val\"])\n",
    "all_results[\"test\"].append(bool_q_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"RTE/train.jsonl\"\n",
    "val_path = data_dir / \"RTE/val.jsonl\"\n",
    "test_path = data_dir / \"RTE/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_rte(row):\n",
    "    premise = str(row[\"premise\"]).strip()\n",
    "    hypothesis = row[\"hypothesis\"]\n",
    "    label = row[\"label\"]\n",
    "    res = f\"{premise} {hypothesis}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_rte(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_rte, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_rte(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_rte(train_path, val_path, test_path):\n",
    "    train = build_features_rte(train_path)\n",
    "    val = build_features_rte(val_path)\n",
    "    test = build_features_rte(test_path)\n",
    "    clf = fit_rte(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, rte_scores = eval_rte(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.7152140672782875,\n",
       " 'val': 0.46579804560260585,\n",
       " 'test': 0.4715447154471545}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"RTE\")\n",
    "all_results[\"train\"].append(rte_scores[\"train\"])\n",
    "all_results[\"val\"].append(rte_scores[\"val\"])\n",
    "all_results[\"test\"].append(rte_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WINOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"WINOGRAD/train.jsonl\"\n",
    "val_path = data_dir / \"WINOGRAD/val.jsonl\"\n",
    "test_path = data_dir / \"WINOGRAD/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_winograd(row):\n",
    "    premise = str(row[\"text\"]).strip()\n",
    "    span1 = row[\"target\"][\"span1_text\"]\n",
    "    span2 = row[\"target\"][\"span2_text\"]\n",
    "    label = row[\"label\"]\n",
    "    res = f\"{premise} {span1} {span2}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_winograd(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_winograd, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_winograd(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_winograd(train_path, val_path, test_path):\n",
    "    train = build_features_winograd(train_path)\n",
    "    val = build_features_winograd(val_path)\n",
    "    test = build_features_winograd(test_path)\n",
    "    clf = fit_winograd(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, winograd_scores = eval_winograd(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.5115511551155115,\n",
       " 'val': 0.553921568627451,\n",
       " 'test': 0.6623376623376623}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winograd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"WINOGRAD\")\n",
    "all_results[\"train\"].append(winograd_scores[\"train\"])\n",
    "all_results[\"val\"].append(winograd_scores[\"val\"])\n",
    "all_results[\"test\"].append(winograd_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WiC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"WiC/train.jsonl\"\n",
    "val_path = data_dir / \"WiC/val.jsonl\"\n",
    "test_path = data_dir / \"WiC/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_wic(row):\n",
    "    sentence1 = row[\"sentence1\"].strip()\n",
    "    sentence2 = row[\"sentence2\"].strip()\n",
    "    word = row[\"word\"].strip()\n",
    "    label = row[\"label\"]\n",
    "    res = f\"{sentence1} {sentence2} {word}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_wic(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_wic, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_wic(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_wic(train_path, val_path, test_path):\n",
    "    train = build_features_wic(train_path)\n",
    "    val = build_features_wic(val_path)\n",
    "    test = build_features_wic(test_path)\n",
    "    clf = fit_wic(*train)\n",
    "    return clf, {\n",
    "        \"train\": clf.score(*train),\n",
    "        \"val\": clf.score(*val),\n",
    "        \"test\": clf.score(*test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, wic_scores = eval_wic(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.7103552532123961,\n",
       " 'val': 0.6653733098177542,\n",
       " 'test': 0.6694922228623159}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"WiC\")\n",
    "all_results[\"train\"].append(wic_scores[\"train\"])\n",
    "all_results[\"val\"].append(wic_scores[\"val\"])\n",
    "all_results[\"test\"].append(wic_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"RTE/train.jsonl\"\n",
    "val_path = data_dir / \"RTE/val.jsonl\"\n",
    "test_path = data_dir / \"diagnostics/AX-b-edited.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def build_feature_diagnostics(row):\n",
    "    if row.get(\"sentence1\") is None:\n",
    "        premise = str(row[\"premise\"]).strip()\n",
    "        hypothesis = row[\"hypothesis\"]\n",
    "    else:\n",
    "        premise = str(row[\"sentence1\"]).strip()\n",
    "        hypothesis = row[\"sentence2\"]\n",
    "    label = row[\"label\"]\n",
    "    res = f\"{premise} {hypothesis}\"\n",
    "    return res, label\n",
    "\n",
    "\n",
    "def build_features_diagnostics(path):\n",
    "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
    "        lines = reader.read().split(\"\\n\")\n",
    "        lines = list(map(json.loads, filter(None, lines)))\n",
    "    res = list(map(build_feature_diagnostics, lines))\n",
    "    texts = list(map(lambda x: x[0], res))\n",
    "    labels = list(map(lambda x: x[1], res))\n",
    "    return vect.transform(texts), labels\n",
    "\n",
    "\n",
    "def fit_diagnostics(train, labels):\n",
    "    clf = LogisticRegression()\n",
    "    return clf.fit(train, labels)\n",
    "\n",
    "\n",
    "def eval_diagnostics(train_path, val_path, test_path):\n",
    "    train = build_features_diagnostics(train_path)\n",
    "    val = build_features_diagnostics(val_path)\n",
    "    test = build_features_diagnostics(test_path)\n",
    "    clf = fit_diagnostics(*train)\n",
    "    return clf, {\n",
    "        \"train\": matthews_corrcoef(train[1], clf.predict(train[0])),\n",
    "        \"val\": matthews_corrcoef(val[1], clf.predict(val[0])),\n",
    "        \"test\": matthews_corrcoef(test[1], clf.predict(test[0]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, diagnostics_scores = eval_diagnostics(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.4294719661883857,\n",
       " 'val': -0.06835232958984723,\n",
       " 'test': 0.05974021843803689}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"diagnostics\")\n",
    "all_results[\"train\"].append(diagnostics_scores[\"train\"])\n",
    "all_results[\"val\"].append(diagnostics_scores[\"val\"])\n",
    "all_results[\"test\"].append(diagnostics_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReCoRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"ReCoRD/train.jsonl\"\n",
    "val_path = data_dir / \"ReCoRD/dev.jsonl\"\n",
    "test_path = data_dir / \"ReCoRD/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jsonlines\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Official evaluation script for ReCoRD v1.0.\n",
    "(Some functions are adopted from the SQuAD evaluation script.)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "def evaluate(dataset, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    correct_ids = []\n",
    "    for prediction, passage in tqdm_notebook(zip(predictions, dataset)):\n",
    "        prediction = prediction[\"label\"]\n",
    "        for qa in passage['qas']:\n",
    "            total += 1\n",
    "            ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "\n",
    "            _exact_match = metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "            if int(_exact_match) == 1:\n",
    "                correct_ids.append(qa['idx'])\n",
    "            exact_match += _exact_match\n",
    "\n",
    "            f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = exact_match / total\n",
    "    f1 = f1 / total\n",
    "    return exact_match, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_record(train_path, val_path, test_path):\n",
    "    return None, {\n",
    "        \"train\": eval_part(train_path),\n",
    "        \"val\": eval_part(val_path),\n",
    "        \"test\": eval_part(test_path)\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_part(path):\n",
    "    with jsonlines.open(path) as reader:\n",
    "        lines = list(reader)\n",
    "    preds = []\n",
    "    for row in tqdm_notebook(lines, total=len(lines), leave=False):\n",
    "        pred = get_row_pred(row)\n",
    "        preds.append({\n",
    "            \"idx\": row[\"idx\"],\n",
    "            \"label\": pred\n",
    "        })\n",
    "    return evaluate(lines, preds)\n",
    "\n",
    "\n",
    "def get_row_pred(row, top_n=5):\n",
    "    text = vect.transform([row[\"passage\"][\"text\"].replace(\"\\n@highlight\\n\", \" \")])\n",
    "    res = []\n",
    "    words = [\n",
    "        row[\"passage\"][\"text\"][x[\"start\"]: x[\"end\"]]\n",
    "        for x in row[\"passage\"][\"entities\"]]\n",
    "    for line in row[\"qas\"]:\n",
    "        line_candidates = []\n",
    "        for word in words:\n",
    "            line_candidates.append(line[\"query\"].replace(\"@placeholder\", word))\n",
    "        cos = cosine_similarity(text, vect.transform(line_candidates))\n",
    "        pred = np.array(words)[cos.argsort()[0][-1]]\n",
    "        res.append(pred)\n",
    "    return \" \".join(res).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"source\": \"lenta\", \"passage\": {\"text\": \"Против министра здравоохранения Саратовской области Натальи Мазиной возбуждено уголовное дело. Об этом в пятницу, 17 января,   сообщается на сайте управления Следственного комитета России (СКР) по региону. Против Мазиной и других сотрудников регионального ведомства следствие возбудило дело о злоупотреблении должностными полномочиями. По данным СКР, осенью 2018 года министерство здравоохранения региона заключило три госконтракта на поставку 18 аппаратов УЗИ ненадлежащего качества. Мазина знала об этом, однако воспользовалась своим служебным положением дала указание врачам принять медицинские изделия. Позже глава министерства организовала оплату данного оборудования, причинив городу ущерб в 53 миллиона рублей.\\n@highlight\\nВ Москве врач-анестезиолог изнасиловал пациентку после операции\\n@highlight\\nСкворцова поспорила с Голиковой об «ужасной» оптимизации здравоохранения\\n@highlight\\nРастративший два миллиарда рублей российский экс-чиновник пустился в бега\", \"entities\": [{\"start\": 32, \"end\": 51}, {\"start\": 52, \"end\": 67}, {\"start\": 189, \"end\": 192}, {\"start\": 213, \"end\": 220}, {\"start\": 346, \"end\": 349}, {\"start\": 485, \"end\": 491}, {\"start\": 731, \"end\": 737}, {\"start\": 804, \"end\": 813}, {\"start\": 826, \"end\": 835}]}, \"qas\": [{\"query\": \"Как сообщил «Ленте.ру» источник в правоохранительных органах, @placeholder утверждает, что все расценки и другую информацию она получила от Москвы.\", \"answers\": [{\"start\": 52, \"end\": 67, \"text\": \"Натальи Мазиной\"}, {\"start\": 213, \"end\": 220, \"text\": \"Мазиной\"}, {\"start\": 485, \"end\": 491, \"text\": \"Мазина\"}], \"idx\": 4369}], \"idx\": 4369}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72193), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d59544e0c9414dbdc71d0dea82929a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7577), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b7d70e4e944974a5fb174ce3eceff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ada9a1b3f42abaaed97c3d12d30ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf, record_scores = eval_record(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (0.20824733699943207, 0.2263773335525391),\n",
       " 'val': (0.22951036030091065, 0.23539879459768612),\n",
       " 'test': (0.25151598676957, 0.2564189898671709)}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"Record\")\n",
    "all_results[\"train\"].append(record_scores[\"train\"])\n",
    "all_results[\"val\"].append(record_scores[\"val\"])\n",
    "all_results[\"test\"].append(record_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir / \"MultiRC/train.jsonl\"\n",
    "val_path = data_dir / \"MultiRC/val.jsonl\"\n",
    "test_path = data_dir / \"MultiRC/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "class MultiRCMetrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def per_question_metrics(dataset, output_map):\n",
    "        P = []\n",
    "        R = []\n",
    "        for n, example in enumerate(dataset):\n",
    "                    predictedAns = example\n",
    "                    correctAns = output_map[n]\n",
    "                    predictCount = sum(predictedAns)\n",
    "                    correctCount = sum(correctAns)\n",
    "                    assert math.ceil(sum(predictedAns)) == sum(predictedAns), \"sum of the scores: \" + str(sum(predictedAns))\n",
    "                    agreementCount = sum([a * b for (a, b) in zip(correctAns, predictedAns)])\n",
    "                    p1 = (1.0 * agreementCount / predictCount) if predictCount > 0.0 else 1.0\n",
    "                    r1 = (1.0 * agreementCount / correctCount) if correctCount > 0.0 else 1.0\n",
    "                    P.append(p1)\n",
    "                    R.append(r1)\n",
    "\n",
    "        pAvg = Measures.avg(P)\n",
    "        rAvg = Measures.avg(R)\n",
    "        f1Avg = 2 * Measures.avg(R) * Measures.avg(P) / (Measures.avg(P) + Measures.avg(R))\n",
    "        return [pAvg, rAvg, f1Avg]\n",
    "\n",
    "    @staticmethod\n",
    "    def exact_match_metrics_origin(dataset, output_map, delta):\n",
    "        EM = []\n",
    "        for n, example in enumerate(dataset):\n",
    "            predictedAns = example\n",
    "            correctAns = output_map[n]\n",
    "\n",
    "            em = 1.0 if sum([abs(i - j) for i, j in zip(correctAns, predictedAns)]) <= delta  else 0.0\n",
    "            EM.append(em)\n",
    "        return Measures.avg(EM)\n",
    "\n",
    "    @staticmethod\n",
    "    def exact_match_simple(dataset, output_map):\n",
    "        EM = []\n",
    "        for n, example in enumerate(dataset):\n",
    "            predictedAns = example\n",
    "            correctAns = output_map[n]\n",
    "            if predictedAns == correctAns:\n",
    "                em = 1\n",
    "            else:\n",
    "                em = 0\n",
    "            EM.append(em)\n",
    "        return sum(EM)/len(EM)\n",
    "\n",
    "    @staticmethod\n",
    "    def per_dataset_metric(dataset, output_map):\n",
    "        \"\"\"\n",
    "        dataset = [[0,1,1], [0,1]]\n",
    "        output_map = [[0,1,0], [0,1]]\n",
    "        \"\"\"\n",
    "        agreementCount = 0\n",
    "        correctCount = 0\n",
    "        predictCount = 0\n",
    "        for n, example in enumerate(dataset):\n",
    "                predictedAns = example\n",
    "                correctAns = output_map[n]\n",
    "                predictCount += sum(predictedAns)\n",
    "                correctCount += sum(correctAns)\n",
    "                agreementCount += sum([a * b for (a, b) in zip(correctAns, predictedAns)])\n",
    "\n",
    "        p1 = (1.0 * agreementCount / predictCount) if predictCount > 0.0 else 1.0\n",
    "        r1 = (1.0 * agreementCount / correctCount) if correctCount > 0.0 else 1.0\n",
    "        return [p1, r1, 2 * r1 * p1 / (p1 + r1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def avg(l):\n",
    "        return functools.reduce(lambda x, y: x + y, l) / len(l)\n",
    "\n",
    "    \n",
    "def multiRC_metrics(pred, labels):\n",
    "    metrics = MultiRCMetrics()\n",
    "    em = metrics.exact_match_simple(pred, labels)\n",
    "    em0 = metrics.exact_match_metrics_origin(pred, labels, 0)\n",
    "    f1 = metrics.per_dataset_metric(pred, labels)\n",
    "    f1a = f1[-1]\n",
    "    return em0, f1a\n",
    "\n",
    "\n",
    "Measures = MultiRCMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multirc(train_path, val_path, test_path):\n",
    "    return None, {\n",
    "        \"train\": eval_part_multirc(train_path),\n",
    "        \"val\": eval_part_multirc(val_path),\n",
    "        \"test\": eval_part_multirc(test_path)\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_part_multirc(path):\n",
    "    with jsonlines.open(path) as reader:\n",
    "        lines = list(reader)\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for row in lines:\n",
    "        pred, lbls = get_row_pred_multirc(row)\n",
    "        preds.extend(pred)\n",
    "        labels.extend(lbls)\n",
    "    return multiRC_metrics(preds, labels)\n",
    "\n",
    "\n",
    "def get_row_pred_multirc(row, top_n=5):\n",
    "    text = vect.transform([row[\"passage\"][\"text\"]])\n",
    "    res = []\n",
    "    labels = []\n",
    "    for line in row[\"passage\"][\"questions\"]:\n",
    "        line_answers = []\n",
    "        line_labels = []\n",
    "        for answ in line[\"answers\"]:\n",
    "            line_labels.append(answ[\"label\"])\n",
    "            answ = f\"{line['question']} {answ['text']}\"\n",
    "            line_answers.append(answ)\n",
    "        cos = cosine_similarity(text, vect.transform(line_answers))\n",
    "        pred = cos.argsort()[0][-2:]\n",
    "        pred = [int(idx in pred) for idx in range(len(line[\"answers\"]))]\n",
    "        res.append(pred)\n",
    "        labels.append(line_labels)\n",
    "    return res, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, multirc_scores = eval_multirc(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (0.2140077821011673, 0.5475732090384031),\n",
       " 'val': (0.20982986767485823, 0.5207215992198928),\n",
       " 'test': (0.24434638720353005, 0.5895127875410773)}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multirc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"name\"].append(\"MultiRC\")\n",
    "all_results[\"train\"].append(multirc_scores[\"train\"])\n",
    "all_results[\"val\"].append(multirc_scores[\"val\"])\n",
    "all_results[\"test\"].append(multirc_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COPA</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CommitmentBank</td>\n",
       "      <td>0.742009</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.452055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BoolQ</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.664407</td>\n",
       "      <td>0.684746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTE</td>\n",
       "      <td>0.715214</td>\n",
       "      <td>0.465798</td>\n",
       "      <td>0.471545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WINOGRAD</td>\n",
       "      <td>0.511551</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.662338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WiC</td>\n",
       "      <td>0.710355</td>\n",
       "      <td>0.665373</td>\n",
       "      <td>0.669492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>diagnostics</td>\n",
       "      <td>0.429472</td>\n",
       "      <td>-0.0683523</td>\n",
       "      <td>0.0597402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Record</td>\n",
       "      <td>(0.20824733699943207, 0.2263773335525391)</td>\n",
       "      <td>(0.22951036030091065, 0.23539879459768612)</td>\n",
       "      <td>(0.25151598676957, 0.2564189898671709)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MultiRC</td>\n",
       "      <td>(0.2140077821011673, 0.5475732090384031)</td>\n",
       "      <td>(0.20982986767485823, 0.5207215992198928)</td>\n",
       "      <td>(0.24434638720353005, 0.5895127875410773)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                      train  \\\n",
       "0            COPA                                      0.775   \n",
       "1  CommitmentBank                                   0.742009   \n",
       "2           BoolQ                                   0.732143   \n",
       "3             RTE                                   0.715214   \n",
       "4        WINOGRAD                                   0.511551   \n",
       "5             WiC                                   0.710355   \n",
       "6     diagnostics                                   0.429472   \n",
       "7          Record  (0.20824733699943207, 0.2263773335525391)   \n",
       "8         MultiRC   (0.2140077821011673, 0.5475732090384031)   \n",
       "\n",
       "                                          val  \\\n",
       "0                                        0.45   \n",
       "1                                    0.522727   \n",
       "2                                    0.664407   \n",
       "3                                    0.465798   \n",
       "4                                    0.553922   \n",
       "5                                    0.665373   \n",
       "6                                  -0.0683523   \n",
       "7  (0.22951036030091065, 0.23539879459768612)   \n",
       "8   (0.20982986767485823, 0.5207215992198928)   \n",
       "\n",
       "                                        test  \n",
       "0                                      0.486  \n",
       "1                                   0.452055  \n",
       "2                                   0.684746  \n",
       "3                                   0.471545  \n",
       "4                                   0.662338  \n",
       "5                                   0.669492  \n",
       "6                                  0.0597402  \n",
       "7     (0.25151598676957, 0.2564189898671709)  \n",
       "8  (0.24434638720353005, 0.5895127875410773)  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results.csv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
