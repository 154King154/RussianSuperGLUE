{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uu9C3-LgY-zx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "import os\n",
    "#!pip install transformers allennlp\n",
    "#!pip install jsondiff\n",
    "#!pip install pyhocon\n",
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WtjPCJAckV8"
   },
   "source": [
    "**Пример запуска baseline модели для Russian-SUPERGlue**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81SBfVUmZB7u"
   },
   "outputs": [],
   "source": [
    "#!unzip jiant-russian.zip\n",
    "os.chdir('./jiant-russian/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DP1fs9zdfaij"
   },
   "source": [
    "**Настройка config'а**\n",
    "\n",
    "\n",
    "Для работы pipelin'а необходимо указать путь к данным и путь для сохранения модели в  `user_config.sh`:\n",
    "\n",
    "1) `export JIANT_DATA_DIR=./combined/` - путь к данным для тасков. По умолчанию указана папка `combined`, которую необходимо поместить в корневую директорию.\n",
    "\n",
    "2) `export JIANT_PROJECT_PREFIX=./model_dir/` - путь, где сохраняются модели, логи, результаты предсказания. По умолчанию указана `./model_dir/`.\n",
    "\n",
    "Параметры, специфичные для модели и параметры обучения указываются в конфиге модели: `jiant/config/superglue_bert.conf`. \n",
    "\n",
    "\n",
    "Например, там задается:\n",
    "\n",
    " `input_module` - используемая бертовская модель (по умолчанию `\"DeepPavlov/rubert-base-cased-conversational\"`)\n",
    "\n",
    " `exp_name` - короткое название модели, по умолчанию `rubert`\n",
    "\n",
    " `max_val, max_epochs, learning_rate` и другие.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Данные** \n",
    "\n",
    "Данные можно скачать с сайта [RussianGLUE](https://russiansuperglue.com/tasks/). По [ссылке](https://russiansuperglue.com/tasks/download) можно скачать данные для всех таксков разом. Дальше достаточно будет разархивировать архив и указать путь к нему в качестве `export JIANT_PROJECT_PREFIX=` в конфиге.\n",
    "\n",
    "**Структура данных**\n",
    "\n",
    "Данные для каждого из заданий должны лежать в подпапке с аналогичным названием в `export JIANT_PROJECT_PREFIX=` (в [общем архиве с данными](https://russiansuperglue.com/tasks/download) как раз такая струтура): \n",
    "\n",
    "1) DaNetQa\n",
    "\n",
    "2) LiDiRus\n",
    "\n",
    "3) MuSeRC\n",
    "\n",
    "4) PARus\n",
    "\n",
    "5) RCB\n",
    "\n",
    "6) RuCoS\n",
    "\n",
    "7) RUSSE\n",
    "\n",
    "8) RWSD\n",
    "\n",
    "9) TERRa\n",
    "\n",
    "Для запуска всех тасков кроме диагностического LiDiRus в папке должны находиться `train.jsonl, val.jsonl, test.jsonl`. Для LiDiRus в соответствующую папку надо положить `LiDiRus.jsonl`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0vzVwEdDN7d"
   },
   "source": [
    "**Запуск baseline**\n",
    "\n",
    "Запуска baselin'ов осуществляется с помощью скрипта `./scripts/russian-superglue-baselines.sh`. Для обучения модели и получения предсказаний необходимо запустить скрипт с указанием необходимого таска (предсказания для диагностического `LiDiRus` вычисляются при обучении на TERRa, то есть при запуске terra модель делает предсказания как для terra_test, так и для lidirus.\n",
    "\n",
    "\n",
    "**Usage:**\n",
    "```\n",
    "   ./scripts/superglue-baselines.sh ${TASK} ${GPU_ID} ${SEED}\n",
    "   - TASK: one of {\"danetqa\", \"rcb\", \"parus\", \"muserc\", \"rucos\", \"terra\", \"russe\", \"rwsd\"},\n",
    "\n",
    "   - GPU_ID: GPU to use, or -1 for CPU. Defaults to 0.\n",
    "   - SEED: random seed. Defaults to 111.\n",
    "```\n",
    "\n",
    "Результаты сохраняются в `model_dir/exp_name/task_name`. \n",
    "\n",
    "Скоры модели на валидации по всем отработанным таскам записываются в `model_dir/exp_name/results.tsv`.\n",
    "\n",
    "Ниже приведен пример базового запуска для задания `TERRa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BVwaqEzyZMTO",
    "outputId": "314c6267-5357-42f2-a57d-c7befeac2fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05 07:43:45 AM: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "06/05 07:43:45 AM: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "06/05 07:43:45 AM: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "06/05 07:43:45 AM: instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "06/05 07:43:45 AM: instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "06/05 07:43:45 AM: instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "06/05 07:43:45 AM: instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "06/05 07:43:46 AM: PyTorch version 1.5.0+cu101 available.\n",
      "2020-06-05 07:43:46.298691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "06/05 07:43:47 AM: TensorFlow version 2.2.0 available.\n",
      "06/05 07:43:47 AM: Loading config from jiant/config/superglue_bert.conf\n",
      "06/05 07:43:47 AM: Config overrides: random_seed = 111, cuda = 0, run_name = terra, pretrain_tasks = \"terra\", target_tasks = \"terra,lidirus\", do_pretrain = 1, do_target_task_training = 0, do_full_eval = 1, batch_size = 4, val_interval = 625\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "06/05 07:43:48 AM: Git branch: \n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "06/05 07:43:48 AM: Git SHA: \n",
      "06/05 07:43:48 AM: Parsed args: \n",
      "{\n",
      "  \"batch_size\": 4,\n",
      "  \"classifier\": \"log_reg\",\n",
      "  \"cuda\": 0,\n",
      "  \"do_target_task_training\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dropout_embs\": 0.1,\n",
      "  \"exp_dir\": \"./model_dir//rubert/\",\n",
      "  \"exp_name\": \"rubert\",\n",
      "  \"input_module\": \"DeepPavlov/rubert-base-cased-conversational\",\n",
      "  \"local_log_path\": \"./model_dir//rubert/terra/log.log\",\n",
      "  \"lr\": 1e-05,\n",
      "  \"lr_patience\": 4,\n",
      "  \"max_epochs\": 10,\n",
      "  \"max_seq_len\": 256,\n",
      "  \"max_vals\": 10,\n",
      "  \"min_lr\": 0.0,\n",
      "  \"optimizer\": \"bert_adam\",\n",
      "  \"pair_attn\": 0,\n",
      "  \"patience\": 20,\n",
      "  \"pretrain_tasks\": \"terra\",\n",
      "  \"random_seed\": 111,\n",
      "  \"remote_log_name\": \"rubert__terra\",\n",
      "  \"run_dir\": \"./model_dir//rubert/terra\",\n",
      "  \"run_name\": \"terra\",\n",
      "  \"s2s\": {\n",
      "    \"attention\": \"none\"\n",
      "  },\n",
      "  \"sent_enc\": \"none\",\n",
      "  \"sep_embs_for_skip\": 1,\n",
      "  \"target_tasks\": \"terra,lidirus\",\n",
      "  \"transfer_paradigm\": \"finetune\",\n",
      "  \"transformers_output_mode\": \"top\",\n",
      "  \"val_interval\": 625,\n",
      "  \"write_preds\": \"val,test\",\n",
      "  \"write_strict_glue_format\": 1\n",
      "}\n",
      "06/05 07:43:48 AM: Saved config to ./model_dir//rubert/terra/params.conf\n",
      "06/05 07:43:48 AM: Using random seed 111\n",
      "06/05 07:43:48 AM: Using GPU 0\n",
      "06/05 07:43:48 AM: Selecting tokenizer\n",
      "06/05 07:43:48 AM: DeepPavlov/rubert-base-cased-conversational\n",
      "06/05 07:43:48 AM: Loading tasks...\n",
      "06/05 07:43:48 AM: Writing pre-preprocessed tasks to ./model_dir//rubert/\n",
      "06/05 07:43:48 AM: \tCreating task lidirus from scratch.\n",
      "06/05 07:43:48 AM: \tLoading Tokenizer DeepPavlov/rubert-base-cased-conversational\n",
      "06/05 07:43:48 AM: Model name 'DeepPavlov/rubert-base-cased-conversational' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'DeepPavlov/rubert-base-cased-conversational' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "06/05 07:43:49 AM: Lock 140069387274840 acquired on /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3.lock\n",
      "06/05 07:43:49 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpk0rktn9_\n",
      "Downloading: 100% 1.40M/1.40M [00:00<00:00, 2.54MB/s]\n",
      "06/05 07:43:50 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt in cache at /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:43:50 AM: creating metadata file for /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:43:50 AM: Lock 140069387274840 released on /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3.lock\n",
      "06/05 07:43:50 AM: Lock 140069387274840 acquired on /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n",
      "06/05 07:43:50 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpioub8sew\n",
      "Downloading: 100% 112/112 [00:00<00:00, 75.3kB/s]\n",
      "06/05 07:43:51 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json in cache at /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:43:51 AM: creating metadata file for /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:43:51 AM: Lock 140069387274840 released on /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n",
      "06/05 07:43:51 AM: Lock 140069387274840 acquired on /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n",
      "06/05 07:43:51 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3cged48w\n",
      "Downloading: 100% 24.0/24.0 [00:00<00:00, 19.0kB/s]\n",
      "06/05 07:43:52 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json in cache at /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:43:52 AM: creating metadata file for /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:43:52 AM: Lock 140069387274840 released on /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n",
      "06/05 07:43:52 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt from cache at /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:43:52 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/added_tokens.json from cache at None\n",
      "06/05 07:43:52 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json from cache at /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:43:52 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json from cache at /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:43:52 AM: Rubert tokenizer has been chosen.\n",
      "06/05 07:43:52 AM: \tFinished loading LiDiRus data.\n",
      "06/05 07:43:52 AM: \tFinished creating score functions for diagnostic data.\n",
      "06/05 07:43:52 AM: \tTask 'lidirus': |train|=1104 |val|=1104 |test|=1104\n",
      "06/05 07:43:52 AM: \tCreating task terra from scratch.\n",
      "06/05 07:43:58 AM: \tFinished loading TERRa.\n",
      "06/05 07:43:58 AM: \tTask 'terra': |train|=2616 |val|=307 |test|=3198\n",
      "06/05 07:43:58 AM: \tFinished loading tasks: lidirus terra.\n",
      "06/05 07:43:58 AM: In building vocab\n",
      "06/05 07:43:58 AM: ./model_dir//rubert/\n",
      "06/05 07:43:58 AM: \tBuilding vocab from scratch.\n",
      "06/05 07:43:58 AM: \tCounting units for task lidirus.\n",
      "06/05 07:43:58 AM: \tCounting units for task terra.\n",
      "06/05 07:43:58 AM: In add_transformers_vocab\n",
      "06/05 07:43:58 AM: DeepPavlov/rubert-base-cased-conversational\n",
      "06/05 07:43:58 AM: Model name 'DeepPavlov/rubert-base-cased-conversational' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'DeepPavlov/rubert-base-cased-conversational' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "06/05 07:44:00 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt from cache at /root/.cache/torch/transformers/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:44:00 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/added_tokens.json from cache at None\n",
      "06/05 07:44:00 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json from cache at /root/.cache/torch/transformers/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:44:00 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json from cache at /root/.cache/torch/transformers/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:44:00 AM: Added transformers vocab (DeepPavlov/rubert-base-cased-conversational): 100792 tokens\n",
      "06/05 07:44:00 AM: \tVocab namespace tokens: size 21521\n",
      "06/05 07:44:00 AM: \tVocab namespace chars: size 152\n",
      "06/05 07:44:00 AM: \tVocab namespace DeepPavlov/rubert-base-cased-conversational: size 100794\n",
      "06/05 07:44:00 AM: \tFinished building vocab.\n",
      "06/05 07:44:00 AM: in mpi\n",
      "06/05 07:44:00 AM: \tTask lidirus (train): Indexing from scratch.\n",
      "06/05 07:44:01 AM: \tTask lidirus (train): Saved 1104 instances to ./model_dir//rubert/preproc/lidirus__train_data\n",
      "06/05 07:44:01 AM: \tTask lidirus (val): Indexing from scratch.\n",
      "06/05 07:44:02 AM: \tTask lidirus (val): Saved 1104 instances to ./model_dir//rubert/preproc/lidirus__val_data\n",
      "06/05 07:44:02 AM: \tTask lidirus (test): Indexing from scratch.\n",
      "06/05 07:44:02 AM: \tTask lidirus (test): Saved 1104 instances to ./model_dir//rubert/preproc/lidirus__test_data\n",
      "06/05 07:44:02 AM: \tTask terra (train): Indexing from scratch.\n",
      "06/05 07:44:03 AM: \tTask terra (train): Saved 2616 instances to ./model_dir//rubert/preproc/terra__train_data\n",
      "06/05 07:44:03 AM: \tTask terra (val): Indexing from scratch.\n",
      "06/05 07:44:03 AM: \tTask terra (val): Saved 307 instances to ./model_dir//rubert/preproc/terra__val_data\n",
      "06/05 07:44:03 AM: \tTask terra (test): Indexing from scratch.\n",
      "06/05 07:44:04 AM: \tTask terra (test): Saved 3198 instances to ./model_dir//rubert/preproc/terra__test_data\n",
      "06/05 07:44:04 AM: \tFinished indexing tasks\n",
      "06/05 07:44:04 AM: \tCreating trimmed target-only version of lidirus train.\n",
      "06/05 07:44:04 AM: \tCreating trimmed pretraining-only version of terra train.\n",
      "06/05 07:44:04 AM: \tCreating trimmed target-only version of terra train.\n",
      "06/05 07:44:04 AM: \t  Training on terra\n",
      "06/05 07:44:04 AM: \t  Evaluating on terra, lidirus\n",
      "06/05 07:44:04 AM: \tFinished loading tasks in 16.192s\n",
      "06/05 07:44:04 AM: \t Tasks: ['lidirus', 'terra']\n",
      "06/05 07:44:04 AM: Building model...\n",
      "06/05 07:44:04 AM: Using BERT model (DeepPavlov/rubert-base-cased-conversational).\n",
      "06/05 07:44:05 AM: Lock 140069388047360 acquired on ./model_dir//rubert/transformers_cache/c378463208d89be9cfdd0b6445748fe663f0d026d9d97da54d10eec4139c5ac1.41d1cb30da8abef9028a44a17bd9c152daca0bd46e409bc271f324a28d109450.lock\n",
      "06/05 07:44:05 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/config.json not found in cache or force_download set to True, downloading to /content/jiant-russian/model_dir/rubert/transformers_cache/tmpro1f2o83\n",
      "Downloading: 100% 642/642 [00:00<00:00, 517kB/s]\n",
      "06/05 07:44:05 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/config.json in cache at ./model_dir//rubert/transformers_cache/c378463208d89be9cfdd0b6445748fe663f0d026d9d97da54d10eec4139c5ac1.41d1cb30da8abef9028a44a17bd9c152daca0bd46e409bc271f324a28d109450\n",
      "06/05 07:44:05 AM: creating metadata file for ./model_dir//rubert/transformers_cache/c378463208d89be9cfdd0b6445748fe663f0d026d9d97da54d10eec4139c5ac1.41d1cb30da8abef9028a44a17bd9c152daca0bd46e409bc271f324a28d109450\n",
      "06/05 07:44:05 AM: Lock 140069388047360 released on ./model_dir//rubert/transformers_cache/c378463208d89be9cfdd0b6445748fe663f0d026d9d97da54d10eec4139c5ac1.41d1cb30da8abef9028a44a17bd9c152daca0bd46e409bc271f324a28d109450.lock\n",
      "06/05 07:44:05 AM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/config.json from cache at ./model_dir//rubert/transformers_cache/c378463208d89be9cfdd0b6445748fe663f0d026d9d97da54d10eec4139c5ac1.41d1cb30da8abef9028a44a17bd9c152daca0bd46e409bc271f324a28d109450\n",
      "06/05 07:44:05 AM: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "06/05 07:44:06 AM: Lock 140069387795760 acquired on ./model_dir//rubert/transformers_cache/c73271f82ca8738e8113b08edd9626107918d0c4a8b58241c91f86a6dff83f6a.18acd9d402cf109f56c6ab3701ff633a49fe3cd35a4c867ff150133446f60c33.lock\n",
      "06/05 07:44:06 AM: https://cdn.huggingface.co/DeepPavlov/rubert-base-cased-conversational/pytorch_model.bin not found in cache or force_download set to True, downloading to /content/jiant-russian/model_dir/rubert/transformers_cache/tmpxg9xnq29\n",
      "Downloading: 100% 711M/711M [00:13<00:00, 52.3MB/s]\n",
      "06/05 07:44:19 AM: storing https://cdn.huggingface.co/DeepPavlov/rubert-base-cased-conversational/pytorch_model.bin in cache at ./model_dir//rubert/transformers_cache/c73271f82ca8738e8113b08edd9626107918d0c4a8b58241c91f86a6dff83f6a.18acd9d402cf109f56c6ab3701ff633a49fe3cd35a4c867ff150133446f60c33\n",
      "06/05 07:44:19 AM: creating metadata file for ./model_dir//rubert/transformers_cache/c73271f82ca8738e8113b08edd9626107918d0c4a8b58241c91f86a6dff83f6a.18acd9d402cf109f56c6ab3701ff633a49fe3cd35a4c867ff150133446f60c33\n",
      "06/05 07:44:19 AM: Lock 140069387795760 released on ./model_dir//rubert/transformers_cache/c73271f82ca8738e8113b08edd9626107918d0c4a8b58241c91f86a6dff83f6a.18acd9d402cf109f56c6ab3701ff633a49fe3cd35a4c867ff150133446f60c33.lock\n",
      "06/05 07:44:19 AM: loading weights file https://cdn.huggingface.co/DeepPavlov/rubert-base-cased-conversational/pytorch_model.bin from cache at ./model_dir//rubert/transformers_cache/c73271f82ca8738e8113b08edd9626107918d0c4a8b58241c91f86a6dff83f6a.18acd9d402cf109f56c6ab3701ff633a49fe3cd35a4c867ff150133446f60c33\n",
      "06/05 07:44:24 AM: Model name 'DeepPavlov/rubert-base-cased-conversational' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'DeepPavlov/rubert-base-cased-conversational' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "06/05 07:44:24 AM: Lock 140069051181600 acquired on ./model_dir//rubert/transformers_cache/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3.lock\n",
      "06/05 07:44:24 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt not found in cache or force_download set to True, downloading to /content/jiant-russian/model_dir/rubert/transformers_cache/tmp5bcep0ke\n",
      "Downloading: 100% 1.40M/1.40M [00:00<00:00, 2.55MB/s]\n",
      "06/05 07:44:25 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt in cache at ./model_dir//rubert/transformers_cache/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:44:25 AM: creating metadata file for ./model_dir//rubert/transformers_cache/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:44:25 AM: Lock 140069051181600 released on ./model_dir//rubert/transformers_cache/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3.lock\n",
      "06/05 07:44:26 AM: Lock 140069051181096 acquired on ./model_dir//rubert/transformers_cache/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n",
      "06/05 07:44:26 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json not found in cache or force_download set to True, downloading to /content/jiant-russian/model_dir/rubert/transformers_cache/tmpj9grdr3y\n",
      "Downloading: 100% 112/112 [00:00<00:00, 79.3kB/s]\n",
      "06/05 07:44:27 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json in cache at ./model_dir//rubert/transformers_cache/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:44:27 AM: creating metadata file for ./model_dir//rubert/transformers_cache/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:44:27 AM: Lock 140069051181096 released on ./model_dir//rubert/transformers_cache/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4.lock\n",
      "06/05 07:44:27 AM: Lock 140069051181544 acquired on ./model_dir//rubert/transformers_cache/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n",
      "06/05 07:44:27 AM: https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json not found in cache or force_download set to True, downloading to /content/jiant-russian/model_dir/rubert/transformers_cache/tmpf_k8thma\n",
      "Downloading: 100% 24.0/24.0 [00:00<00:00, 18.4kB/s]\n",
      "06/05 07:44:27 AM: storing https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json in cache at ./model_dir//rubert/transformers_cache/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:44:27 AM: creating metadata file for ./model_dir//rubert/transformers_cache/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:44:27 AM: Lock 140069051181544 released on ./model_dir//rubert/transformers_cache/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d.lock\n",
      "06/05 07:44:27 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/vocab.txt from cache at ./model_dir//rubert/transformers_cache/525233877508550dfe1734e3b2662aaa2d1dab37dfbd5a8a84d3fb8c97899b7e.84c7502687c07e22e0a244716bb6783aca6b9a46339df720d495211d4fba05b3\n",
      "06/05 07:44:27 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/added_tokens.json from cache at None\n",
      "06/05 07:44:27 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/special_tokens_map.json from cache at ./model_dir//rubert/transformers_cache/5a7b032e48e98c40dcc8481342513a8c75b25dcddab531e419484ecbc76dbac8.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "06/05 07:44:27 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased-conversational/tokenizer_config.json from cache at ./model_dir//rubert/transformers_cache/ea21d782b30fd814bb82c87cd78231e1e209bb94b8249f7a129252db1bbdbdd1.23dbcd12a881c5aa23ed8b7502b47eedde8257f0130e23919733d1ed9e4ec20d\n",
      "06/05 07:44:28 AM: Initializing parameters\n",
      "06/05 07:44:28 AM: Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.pooler.dense.bias\n",
      "06/05 07:44:28 AM:    _text_field_embedder.model.pooler.dense.weight\n",
      "06/05 07:44:28 AM: \tTask 'lidirus' params: {\n",
      "  \"cls_type\": \"log_reg\",\n",
      "  \"d_hid\": 512,\n",
      "  \"pool_type\": \"max\",\n",
      "  \"d_proj\": 512,\n",
      "  \"shared_pair_attn\": 0,\n",
      "  \"attn\": 0,\n",
      "  \"d_hid_attn\": 512,\n",
      "  \"dropout\": 0.2,\n",
      "  \"cls_loss_fn\": \"softmax\",\n",
      "  \"cls_span_pooling\": \"attn\",\n",
      "  \"edgeprobe_cnn_context\": 0,\n",
      "  \"edgeprobe_symmetric\": 0,\n",
      "  \"use_classifier\": \"lidirus\"\n",
      "}\n",
      "06/05 07:44:28 AM: \tTask 'terra' params: {\n",
      "  \"cls_type\": \"log_reg\",\n",
      "  \"d_hid\": 512,\n",
      "  \"pool_type\": \"max\",\n",
      "  \"d_proj\": 512,\n",
      "  \"shared_pair_attn\": 0,\n",
      "  \"attn\": 0,\n",
      "  \"d_hid_attn\": 512,\n",
      "  \"dropout\": 0.2,\n",
      "  \"cls_loss_fn\": \"softmax\",\n",
      "  \"cls_span_pooling\": \"attn\",\n",
      "  \"edgeprobe_cnn_context\": 0,\n",
      "  \"edgeprobe_symmetric\": 0,\n",
      "  \"use_classifier\": \"terra\"\n",
      "}\n",
      "06/05 07:44:51 AM: Model specification:\n",
      "06/05 07:44:51 AM: MultiTaskModel(\n",
      "  (sent_encoder): SentenceEncoder(\n",
      "    (_text_field_embedder): BertEmbedderModule(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_highway_layer): TimeDistributed(\n",
      "      (_module): Highway(\n",
      "        (_layers): ModuleList()\n",
      "      )\n",
      "    )\n",
      "    (_phrase_layer): NullPhraseLayer()\n",
      "    (_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lidirus_mdl): SingleClassifier(\n",
      "    (pooler): Pooler()\n",
      "    (classifier): Classifier(\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (terra_mdl): SingleClassifier(\n",
      "    (pooler): Pooler()\n",
      "    (classifier): Classifier(\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "06/05 07:44:51 AM: Model parameters:\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Trainable parameter, count 91812096 with torch.Size([119547, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Trainable parameter, count 393216 with torch.Size([512, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Trainable parameter, count 1536 with torch.Size([2, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Trainable parameter, count 2359296 with torch.Size([3072, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Trainable parameter, count 3072 with torch.Size([3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Trainable parameter, count 2359296 with torch.Size([768, 3072])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.pooler.dense.weight: Trainable parameter, count 589824 with torch.Size([768, 768])\n",
      "06/05 07:44:51 AM: \tsent_encoder._text_field_embedder.model.pooler.dense.bias: Trainable parameter, count 768 with torch.Size([768])\n",
      "06/05 07:44:51 AM: \tlidirus_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])\n",
      "06/05 07:44:51 AM: \tlidirus_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])\n",
      "06/05 07:44:51 AM: \tterra_mdl.classifier.classifier.weight: Trainable parameter, count 1536 with torch.Size([2, 768])\n",
      "06/05 07:44:51 AM: \tterra_mdl.classifier.classifier.bias: Trainable parameter, count 2 with torch.Size([2])\n",
      "06/05 07:44:51 AM: Total number of parameters: 177856516 (1.77857e+08)\n",
      "06/05 07:44:51 AM: Number of trainable parameters: 177856516 (1.77857e+08)\n",
      "06/05 07:44:51 AM: Finished building model in 46.868s\n",
      "06/05 07:44:51 AM: Evauluating a target task model on tasks {'lidirus'} without training it in this run. It's up to you to ensure that you are loading parameters that were sufficiently trained for this task.\n",
      "06/05 07:44:51 AM: Will run the following steps for this experiment:\n",
      "Training model on tasks: terra \n",
      "Evaluating model on tasks: terra,lidirus \n",
      "\n",
      "06/05 07:44:51 AM: Training...\n",
      "06/05 07:44:51 AM: patience = 20\n",
      "06/05 07:44:51 AM: val_interval = 625\n",
      "06/05 07:44:51 AM: max_vals = 10\n",
      "06/05 07:44:51 AM: cuda_device = 0\n",
      "06/05 07:44:51 AM: grad_norm = 5.0\n",
      "06/05 07:44:51 AM: grad_clipping = None\n",
      "06/05 07:44:51 AM: lr_decay = 0.99\n",
      "06/05 07:44:51 AM: min_lr = 1e-07\n",
      "06/05 07:44:51 AM: keep_all_checkpoints = 0\n",
      "06/05 07:44:51 AM: val_data_limit = 5000\n",
      "06/05 07:44:51 AM: max_epochs = 10\n",
      "06/05 07:44:51 AM: dec_val_scale = 250\n",
      "06/05 07:44:51 AM: training_data_fraction = 1\n",
      "06/05 07:44:51 AM: accumulation_steps = 1\n",
      "06/05 07:44:51 AM: type = bert_adam\n",
      "06/05 07:44:51 AM: parameter_groups = None\n",
      "06/05 07:44:51 AM: Number of trainable parameters: 177856516\n",
      "06/05 07:44:51 AM: infer_type_and_cast = True\n",
      "06/05 07:44:51 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "06/05 07:44:51 AM: CURRENTLY DEFINED PARAMETERS: \n",
      "06/05 07:44:51 AM: lr = 1e-05\n",
      "06/05 07:44:51 AM: t_total = 6250\n",
      "06/05 07:44:51 AM: warmup = 0.1\n",
      "06/05 07:44:51 AM: instantiating registered subclass bert_adam of <class 'allennlp.training.optimizers.Optimizer'>\n",
      "06/05 07:44:51 AM: type = reduce_on_plateau\n",
      "06/05 07:44:51 AM: instantiating registered subclass reduce_on_plateau of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
      "06/05 07:44:51 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "06/05 07:44:51 AM: CURRENTLY DEFINED PARAMETERS: \n",
      "06/05 07:44:51 AM: mode = max\n",
      "06/05 07:44:51 AM: factor = 0.5\n",
      "06/05 07:44:51 AM: patience = 4\n",
      "06/05 07:44:51 AM: threshold = 0.0001\n",
      "06/05 07:44:51 AM: threshold_mode = abs\n",
      "06/05 07:44:51 AM: verbose = True\n",
      "06/05 07:44:51 AM: load_model=1 but there is not checkpoint.                         Starting training without restoring from a checkpoint.\n",
      "06/05 07:44:51 AM: Training examples per task, before any subsampling: {'terra': 2616}\n",
      "06/05 07:44:51 AM: Beginning training with stopping criteria based on metric: terra_accuracy\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "06/05 07:45:02 AM: Update 26: task terra, steps since last val 26 (total steps = 26): accuracy: 0.4712, terra_loss: 0.8697\n",
      "06/05 07:45:12 AM: Update 54: task terra, steps since last val 54 (total steps = 54): accuracy: 0.4861, terra_loss: 0.8014\n",
      "06/05 07:45:22 AM: Update 82: task terra, steps since last val 82 (total steps = 82): accuracy: 0.5030, terra_loss: 0.7653\n",
      "06/05 07:45:32 AM: Update 111: task terra, steps since last val 111 (total steps = 111): accuracy: 0.5135, terra_loss: 0.7549\n",
      "06/05 07:45:42 AM: Update 140: task terra, steps since last val 140 (total steps = 140): accuracy: 0.4964, terra_loss: 0.7491\n",
      "06/05 07:45:52 AM: Update 169: task terra, steps since last val 169 (total steps = 169): accuracy: 0.5044, terra_loss: 0.7435\n",
      "06/05 07:46:02 AM: Update 199: task terra, steps since last val 199 (total steps = 199): accuracy: 0.5151, terra_loss: 0.7374\n",
      "06/05 07:46:12 AM: Update 227: task terra, steps since last val 227 (total steps = 227): accuracy: 0.5176, terra_loss: 0.7339\n",
      "06/05 07:46:23 AM: Update 256: task terra, steps since last val 256 (total steps = 256): accuracy: 0.5117, terra_loss: 0.7329\n",
      "06/05 07:46:33 AM: Update 285: task terra, steps since last val 285 (total steps = 285): accuracy: 0.5175, terra_loss: 0.7294\n",
      "06/05 07:46:43 AM: Update 314: task terra, steps since last val 314 (total steps = 314): accuracy: 0.5104, terra_loss: 0.7300\n",
      "06/05 07:46:53 AM: Update 342: task terra, steps since last val 342 (total steps = 342): accuracy: 0.5088, terra_loss: 0.7319\n",
      "06/05 07:47:03 AM: Update 370: task terra, steps since last val 370 (total steps = 370): accuracy: 0.5095, terra_loss: 0.7307\n",
      "06/05 07:47:13 AM: Update 400: task terra, steps since last val 400 (total steps = 400): accuracy: 0.5100, terra_loss: 0.7301\n",
      "06/05 07:47:24 AM: Update 429: task terra, steps since last val 429 (total steps = 429): accuracy: 0.5064, terra_loss: 0.7311\n",
      "06/05 07:47:34 AM: Update 458: task terra, steps since last val 458 (total steps = 458): accuracy: 0.5016, terra_loss: 0.7317\n",
      "06/05 07:47:44 AM: Update 487: task terra, steps since last val 487 (total steps = 487): accuracy: 0.5067, terra_loss: 0.7296\n",
      "06/05 07:47:54 AM: Update 516: task terra, steps since last val 516 (total steps = 516): accuracy: 0.5034, terra_loss: 0.7326\n",
      "06/05 07:48:04 AM: Update 545: task terra, steps since last val 545 (total steps = 545): accuracy: 0.5005, terra_loss: 0.7330\n",
      "06/05 07:48:15 AM: Update 574: task terra, steps since last val 574 (total steps = 574): accuracy: 0.4956, terra_loss: 0.7335\n",
      "06/05 07:48:25 AM: Update 604: task terra, steps since last val 604 (total steps = 604): accuracy: 0.4959, terra_loss: 0.7337\n",
      "06/05 07:48:32 AM: ***** Step 625 / Validation 1 *****\n",
      "06/05 07:48:32 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 07:48:32 AM: Validating...\n",
      "06/05 07:48:35 AM: Evaluate: task terra, batch 46 (77): accuracy: 0.5000, terra_loss: 0.7593\n",
      "06/05 07:48:37 AM: Best result seen so far for terra.\n",
      "06/05 07:48:37 AM: Best result seen so far for micro.\n",
      "06/05 07:48:37 AM: Best result seen so far for macro.\n",
      "06/05 07:48:37 AM: Updating LR scheduler:\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "06/05 07:48:37 AM: \tBest result seen so far for macro_avg: 0.495\n",
      "06/05 07:48:37 AM: \t# validation passes without improvement: 0\n",
      "06/05 07:48:37 AM: terra_loss: training: 0.733569 validation: 0.779791\n",
      "06/05 07:48:37 AM: macro_avg: validation: 0.495114\n",
      "06/05 07:48:37 AM: micro_avg: validation: 0.495114\n",
      "06/05 07:48:37 AM: terra_accuracy: training: 0.496800 validation: 0.495114\n",
      "06/05 07:48:37 AM: Global learning rate: 1e-05\n",
      "06/05 07:48:37 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 07:48:57 AM: Update 626: task terra, steps since last val 1 (total steps = 626): accuracy: 0.2500, terra_loss: 0.8345\n",
      "06/05 07:49:08 AM: Update 655: task terra, steps since last val 30 (total steps = 655): accuracy: 0.4917, terra_loss: 0.7762\n",
      "06/05 07:49:19 AM: Update 684: task terra, steps since last val 59 (total steps = 684): accuracy: 0.5593, terra_loss: 0.7138\n",
      "06/05 07:49:29 AM: Update 713: task terra, steps since last val 88 (total steps = 713): accuracy: 0.5881, terra_loss: 0.6979\n",
      "06/05 07:49:39 AM: Update 742: task terra, steps since last val 117 (total steps = 742): accuracy: 0.5983, terra_loss: 0.6779\n",
      "06/05 07:49:50 AM: Update 771: task terra, steps since last val 146 (total steps = 771): accuracy: 0.6079, terra_loss: 0.6703\n",
      "06/05 07:50:00 AM: Update 799: task terra, steps since last val 174 (total steps = 799): accuracy: 0.5948, terra_loss: 0.6760\n",
      "06/05 07:50:10 AM: Update 828: task terra, steps since last val 203 (total steps = 828): accuracy: 0.5961, terra_loss: 0.6739\n",
      "06/05 07:50:20 AM: Update 856: task terra, steps since last val 231 (total steps = 856): accuracy: 0.5942, terra_loss: 0.6707\n",
      "06/05 07:50:30 AM: Update 886: task terra, steps since last val 261 (total steps = 886): accuracy: 0.5872, terra_loss: 0.6783\n",
      "06/05 07:50:41 AM: Update 915: task terra, steps since last val 290 (total steps = 915): accuracy: 0.5991, terra_loss: 0.6742\n",
      "06/05 07:50:51 AM: Update 944: task terra, steps since last val 319 (total steps = 944): accuracy: 0.5972, terra_loss: 0.6755\n",
      "06/05 07:51:01 AM: Update 973: task terra, steps since last val 348 (total steps = 973): accuracy: 0.5884, terra_loss: 0.6779\n",
      "06/05 07:51:11 AM: Update 1002: task terra, steps since last val 377 (total steps = 1002): accuracy: 0.5902, terra_loss: 0.6746\n",
      "06/05 07:51:21 AM: Update 1031: task terra, steps since last val 406 (total steps = 1031): accuracy: 0.5917, terra_loss: 0.6737\n",
      "06/05 07:51:31 AM: Update 1060: task terra, steps since last val 435 (total steps = 1060): accuracy: 0.5954, terra_loss: 0.6712\n",
      "06/05 07:51:42 AM: Update 1089: task terra, steps since last val 464 (total steps = 1089): accuracy: 0.5964, terra_loss: 0.6699\n",
      "06/05 07:51:52 AM: Update 1118: task terra, steps since last val 493 (total steps = 1118): accuracy: 0.5958, terra_loss: 0.6694\n",
      "06/05 07:52:02 AM: Update 1146: task terra, steps since last val 521 (total steps = 1146): accuracy: 0.5964, terra_loss: 0.6675\n",
      "06/05 07:52:12 AM: Update 1175: task terra, steps since last val 550 (total steps = 1175): accuracy: 0.5982, terra_loss: 0.6663\n",
      "06/05 07:52:22 AM: Update 1205: task terra, steps since last val 580 (total steps = 1205): accuracy: 0.5970, terra_loss: 0.6683\n",
      "06/05 07:52:32 AM: Update 1234: task terra, steps since last val 609 (total steps = 1234): accuracy: 0.5969, terra_loss: 0.6679\n",
      "06/05 07:52:38 AM: ***** Step 1250 / Validation 2 *****\n",
      "06/05 07:52:38 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 07:52:38 AM: Validating...\n",
      "06/05 07:52:42 AM: Evaluate: task terra, batch 77 (77): accuracy: 0.4397, terra_loss: 0.7772\n",
      "06/05 07:52:42 AM: Updating LR scheduler:\n",
      "06/05 07:52:42 AM: \tBest result seen so far for macro_avg: 0.495\n",
      "06/05 07:52:42 AM: \t# validation passes without improvement: 1\n",
      "06/05 07:52:42 AM: terra_loss: training: 0.667809 validation: 0.777217\n",
      "06/05 07:52:42 AM: macro_avg: validation: 0.439739\n",
      "06/05 07:52:42 AM: micro_avg: validation: 0.439739\n",
      "06/05 07:52:42 AM: terra_accuracy: training: 0.596000 validation: 0.439739\n",
      "06/05 07:52:42 AM: Global learning rate: 1e-05\n",
      "06/05 07:52:42 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 07:53:04 AM: Update 1251: task terra, steps since last val 1 (total steps = 1251): accuracy: 0.7500, terra_loss: 0.4178\n",
      "06/05 07:53:14 AM: Update 1279: task terra, steps since last val 29 (total steps = 1279): accuracy: 0.6034, terra_loss: 0.6617\n",
      "06/05 07:53:24 AM: Update 1308: task terra, steps since last val 58 (total steps = 1308): accuracy: 0.5991, terra_loss: 0.6501\n",
      "06/05 07:53:34 AM: Update 1334: task terra, steps since last val 84 (total steps = 1334): accuracy: 0.6726, terra_loss: 0.6018\n",
      "06/05 07:53:44 AM: Update 1363: task terra, steps since last val 113 (total steps = 1363): accuracy: 0.6836, terra_loss: 0.5924\n",
      "06/05 07:53:55 AM: Update 1392: task terra, steps since last val 142 (total steps = 1392): accuracy: 0.6989, terra_loss: 0.5769\n",
      "06/05 07:54:05 AM: Update 1420: task terra, steps since last val 170 (total steps = 1420): accuracy: 0.7147, terra_loss: 0.5669\n",
      "06/05 07:54:15 AM: Update 1449: task terra, steps since last val 199 (total steps = 1449): accuracy: 0.7274, terra_loss: 0.5575\n",
      "06/05 07:54:25 AM: Update 1478: task terra, steps since last val 228 (total steps = 1478): accuracy: 0.7281, terra_loss: 0.5566\n",
      "06/05 07:54:36 AM: Update 1506: task terra, steps since last val 256 (total steps = 1506): accuracy: 0.7363, terra_loss: 0.5498\n",
      "06/05 07:54:46 AM: Update 1535: task terra, steps since last val 285 (total steps = 1535): accuracy: 0.7316, terra_loss: 0.5523\n",
      "06/05 07:54:56 AM: Update 1564: task terra, steps since last val 314 (total steps = 1564): accuracy: 0.7365, terra_loss: 0.5511\n",
      "06/05 07:55:06 AM: Update 1593: task terra, steps since last val 343 (total steps = 1593): accuracy: 0.7420, terra_loss: 0.5434\n",
      "06/05 07:55:17 AM: Update 1623: task terra, steps since last val 373 (total steps = 1623): accuracy: 0.7426, terra_loss: 0.5409\n",
      "06/05 07:55:27 AM: Update 1652: task terra, steps since last val 402 (total steps = 1652): accuracy: 0.7432, terra_loss: 0.5450\n",
      "06/05 07:55:37 AM: Update 1681: task terra, steps since last val 431 (total steps = 1681): accuracy: 0.7477, terra_loss: 0.5414\n",
      "06/05 07:55:47 AM: Update 1709: task terra, steps since last val 459 (total steps = 1709): accuracy: 0.7456, terra_loss: 0.5417\n",
      "06/05 07:55:57 AM: Update 1738: task terra, steps since last val 488 (total steps = 1738): accuracy: 0.7490, terra_loss: 0.5363\n",
      "06/05 07:56:08 AM: Update 1767: task terra, steps since last val 517 (total steps = 1767): accuracy: 0.7471, terra_loss: 0.5365\n",
      "06/05 07:56:18 AM: Update 1795: task terra, steps since last val 545 (total steps = 1795): accuracy: 0.7463, terra_loss: 0.5377\n",
      "06/05 07:56:28 AM: Update 1823: task terra, steps since last val 573 (total steps = 1823): accuracy: 0.7443, terra_loss: 0.5402\n",
      "06/05 07:56:38 AM: Update 1852: task terra, steps since last val 602 (total steps = 1852): accuracy: 0.7450, terra_loss: 0.5377\n",
      "06/05 07:56:46 AM: ***** Step 1875 / Validation 3 *****\n",
      "06/05 07:56:46 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 07:56:46 AM: Validating...\n",
      "06/05 07:56:48 AM: Evaluate: task terra, batch 33 (77): accuracy: 0.6061, terra_loss: 0.7219\n",
      "06/05 07:56:51 AM: Best result seen so far for terra.\n",
      "06/05 07:56:51 AM: Best result seen so far for micro.\n",
      "06/05 07:56:51 AM: Best result seen so far for macro.\n",
      "06/05 07:56:51 AM: Updating LR scheduler:\n",
      "06/05 07:56:51 AM: \tBest result seen so far for macro_avg: 0.554\n",
      "06/05 07:56:51 AM: \t# validation passes without improvement: 0\n",
      "06/05 07:56:51 AM: terra_loss: training: 0.536858 validation: 0.775434\n",
      "06/05 07:56:51 AM: macro_avg: validation: 0.553746\n",
      "06/05 07:56:51 AM: micro_avg: validation: 0.553746\n",
      "06/05 07:56:51 AM: terra_accuracy: training: 0.742800 validation: 0.553746\n",
      "06/05 07:56:51 AM: Global learning rate: 1e-05\n",
      "06/05 07:56:51 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 07:57:12 AM: Update 1876: task terra, steps since last val 1 (total steps = 1876): accuracy: 1.0000, terra_loss: 0.3287\n",
      "06/05 07:57:22 AM: Update 1906: task terra, steps since last val 31 (total steps = 1906): accuracy: 0.7339, terra_loss: 0.4993\n",
      "06/05 07:57:32 AM: Update 1935: task terra, steps since last val 60 (total steps = 1935): accuracy: 0.7250, terra_loss: 0.5078\n",
      "06/05 07:57:43 AM: Update 1963: task terra, steps since last val 88 (total steps = 1963): accuracy: 0.7301, terra_loss: 0.5227\n",
      "06/05 07:57:53 AM: Update 1993: task terra, steps since last val 118 (total steps = 1993): accuracy: 0.7648, terra_loss: 0.4842\n",
      "06/05 07:58:04 AM: Update 2022: task terra, steps since last val 147 (total steps = 2022): accuracy: 0.7840, terra_loss: 0.4566\n",
      "06/05 07:58:14 AM: Update 2051: task terra, steps since last val 176 (total steps = 2051): accuracy: 0.7955, terra_loss: 0.4336\n",
      "06/05 07:58:24 AM: Update 2081: task terra, steps since last val 206 (total steps = 2081): accuracy: 0.8046, terra_loss: 0.4143\n",
      "06/05 07:58:34 AM: Update 2109: task terra, steps since last val 234 (total steps = 2109): accuracy: 0.8088, terra_loss: 0.4048\n",
      "06/05 07:58:44 AM: Update 2137: task terra, steps since last val 262 (total steps = 2137): accuracy: 0.8101, terra_loss: 0.4043\n",
      "06/05 07:58:55 AM: Update 2165: task terra, steps since last val 290 (total steps = 2165): accuracy: 0.8129, terra_loss: 0.4007\n",
      "06/05 07:59:05 AM: Update 2194: task terra, steps since last val 319 (total steps = 2194): accuracy: 0.8166, terra_loss: 0.3904\n",
      "06/05 07:59:15 AM: Update 2223: task terra, steps since last val 348 (total steps = 2223): accuracy: 0.8161, terra_loss: 0.3926\n",
      "06/05 07:59:25 AM: Update 2252: task terra, steps since last val 377 (total steps = 2252): accuracy: 0.8210, terra_loss: 0.3875\n",
      "06/05 07:59:35 AM: Update 2281: task terra, steps since last val 406 (total steps = 2281): accuracy: 0.8257, terra_loss: 0.3804\n",
      "06/05 07:59:45 AM: Update 2309: task terra, steps since last val 434 (total steps = 2309): accuracy: 0.8255, terra_loss: 0.3780\n",
      "06/05 07:59:56 AM: Update 2337: task terra, steps since last val 462 (total steps = 2337): accuracy: 0.8279, terra_loss: 0.3748\n",
      "06/05 08:00:06 AM: Update 2366: task terra, steps since last val 491 (total steps = 2366): accuracy: 0.8299, terra_loss: 0.3761\n",
      "06/05 08:00:16 AM: Update 2394: task terra, steps since last val 519 (total steps = 2394): accuracy: 0.8300, terra_loss: 0.3761\n",
      "06/05 08:00:26 AM: Update 2422: task terra, steps since last val 547 (total steps = 2422): accuracy: 0.8314, terra_loss: 0.3737\n",
      "06/05 08:00:36 AM: Update 2451: task terra, steps since last val 576 (total steps = 2451): accuracy: 0.8312, terra_loss: 0.3779\n",
      "06/05 08:00:46 AM: Update 2480: task terra, steps since last val 605 (total steps = 2480): accuracy: 0.8314, terra_loss: 0.3811\n",
      "06/05 08:00:53 AM: ***** Step 2500 / Validation 4 *****\n",
      "06/05 08:00:53 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:00:53 AM: Validating...\n",
      "06/05 08:00:56 AM: Evaluate: task terra, batch 52 (77): accuracy: 0.6010, terra_loss: 0.8962\n",
      "06/05 08:00:58 AM: Best result seen so far for terra.\n",
      "06/05 08:00:58 AM: Best result seen so far for micro.\n",
      "06/05 08:00:58 AM: Best result seen so far for macro.\n",
      "06/05 08:00:58 AM: Updating LR scheduler:\n",
      "06/05 08:00:58 AM: \tBest result seen so far for macro_avg: 0.583\n",
      "06/05 08:00:58 AM: \t# validation passes without improvement: 0\n",
      "06/05 08:00:58 AM: terra_loss: training: 0.377387 validation: 0.937572\n",
      "06/05 08:00:58 AM: macro_avg: validation: 0.583062\n",
      "06/05 08:00:58 AM: micro_avg: validation: 0.583062\n",
      "06/05 08:00:58 AM: terra_accuracy: training: 0.834400 validation: 0.583062\n",
      "06/05 08:00:58 AM: Global learning rate: 1e-05\n",
      "06/05 08:00:58 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:01:19 AM: Update 2501: task terra, steps since last val 1 (total steps = 2501): accuracy: 1.0000, terra_loss: 0.1569\n",
      "06/05 08:01:30 AM: Update 2531: task terra, steps since last val 31 (total steps = 2531): accuracy: 0.9194, terra_loss: 0.2555\n",
      "06/05 08:01:40 AM: Update 2561: task terra, steps since last val 61 (total steps = 2561): accuracy: 0.8770, terra_loss: 0.3526\n",
      "06/05 08:01:50 AM: Update 2590: task terra, steps since last val 90 (total steps = 2590): accuracy: 0.8583, terra_loss: 0.3914\n",
      "06/05 08:02:00 AM: Update 2617: task terra, steps since last val 117 (total steps = 2617): accuracy: 0.8526, terra_loss: 0.4127\n",
      "06/05 08:02:10 AM: Update 2645: task terra, steps since last val 145 (total steps = 2645): accuracy: 0.8707, terra_loss: 0.3642\n",
      "06/05 08:02:21 AM: Update 2674: task terra, steps since last val 174 (total steps = 2674): accuracy: 0.8807, terra_loss: 0.3335\n",
      "06/05 08:02:31 AM: Update 2704: task terra, steps since last val 204 (total steps = 2704): accuracy: 0.8922, terra_loss: 0.3034\n",
      "06/05 08:02:41 AM: Update 2733: task terra, steps since last val 233 (total steps = 2733): accuracy: 0.8970, terra_loss: 0.2886\n",
      "06/05 08:02:51 AM: Update 2763: task terra, steps since last val 263 (total steps = 2763): accuracy: 0.9030, terra_loss: 0.2753\n",
      "06/05 08:03:01 AM: Update 2792: task terra, steps since last val 292 (total steps = 2792): accuracy: 0.9075, terra_loss: 0.2640\n",
      "06/05 08:03:12 AM: Update 2820: task terra, steps since last val 320 (total steps = 2820): accuracy: 0.9109, terra_loss: 0.2590\n",
      "06/05 08:03:22 AM: Update 2849: task terra, steps since last val 349 (total steps = 2849): accuracy: 0.9119, terra_loss: 0.2599\n",
      "06/05 08:03:32 AM: Update 2878: task terra, steps since last val 378 (total steps = 2878): accuracy: 0.9114, terra_loss: 0.2587\n",
      "06/05 08:03:42 AM: Update 2906: task terra, steps since last val 406 (total steps = 2906): accuracy: 0.9126, terra_loss: 0.2547\n",
      "06/05 08:03:52 AM: Update 2934: task terra, steps since last val 434 (total steps = 2934): accuracy: 0.9147, terra_loss: 0.2456\n",
      "06/05 08:04:02 AM: Update 2963: task terra, steps since last val 463 (total steps = 2963): accuracy: 0.9174, terra_loss: 0.2354\n",
      "06/05 08:04:12 AM: Update 2993: task terra, steps since last val 493 (total steps = 2993): accuracy: 0.9168, terra_loss: 0.2432\n",
      "06/05 08:04:22 AM: Update 3023: task terra, steps since last val 523 (total steps = 3023): accuracy: 0.9187, terra_loss: 0.2353\n",
      "06/05 08:04:32 AM: Update 3052: task terra, steps since last val 552 (total steps = 3052): accuracy: 0.9207, terra_loss: 0.2307\n",
      "06/05 08:04:42 AM: Update 3082: task terra, steps since last val 582 (total steps = 3082): accuracy: 0.9218, terra_loss: 0.2264\n",
      "06/05 08:04:52 AM: Update 3111: task terra, steps since last val 611 (total steps = 3111): accuracy: 0.9243, terra_loss: 0.2195\n",
      "06/05 08:04:57 AM: ***** Step 3125 / Validation 5 *****\n",
      "06/05 08:04:57 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:04:57 AM: Validating...\n",
      "06/05 08:05:02 AM: Best result seen so far for terra.\n",
      "06/05 08:05:02 AM: Best result seen so far for micro.\n",
      "06/05 08:05:02 AM: Best result seen so far for macro.\n",
      "06/05 08:05:02 AM: Updating LR scheduler:\n",
      "06/05 08:05:02 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:05:02 AM: \t# validation passes without improvement: 0\n",
      "06/05 08:05:02 AM: terra_loss: training: 0.218808 validation: 1.651417\n",
      "06/05 08:05:02 AM: macro_avg: validation: 0.654723\n",
      "06/05 08:05:02 AM: micro_avg: validation: 0.654723\n",
      "06/05 08:05:02 AM: terra_accuracy: training: 0.924000 validation: 0.654723\n",
      "06/05 08:05:02 AM: Global learning rate: 1e-05\n",
      "06/05 08:05:02 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:05:23 AM: Update 3126: task terra, steps since last val 1 (total steps = 3126): accuracy: 1.0000, terra_loss: 0.0375\n",
      "06/05 08:05:34 AM: Update 3155: task terra, steps since last val 30 (total steps = 3155): accuracy: 0.9417, terra_loss: 0.2163\n",
      "06/05 08:05:44 AM: Update 3184: task terra, steps since last val 59 (total steps = 3184): accuracy: 0.9280, terra_loss: 0.2559\n",
      "06/05 08:05:54 AM: Update 3214: task terra, steps since last val 89 (total steps = 3214): accuracy: 0.9354, terra_loss: 0.2394\n",
      "06/05 08:06:04 AM: Update 3245: task terra, steps since last val 120 (total steps = 3245): accuracy: 0.9417, terra_loss: 0.2086\n",
      "06/05 08:06:14 AM: Update 3272: task terra, steps since last val 147 (total steps = 3272): accuracy: 0.9473, terra_loss: 0.1917\n",
      "06/05 08:06:24 AM: Update 3301: task terra, steps since last val 176 (total steps = 3301): accuracy: 0.9517, terra_loss: 0.1724\n",
      "06/05 08:06:35 AM: Update 3331: task terra, steps since last val 206 (total steps = 3331): accuracy: 0.9563, terra_loss: 0.1571\n",
      "06/05 08:06:45 AM: Update 3361: task terra, steps since last val 236 (total steps = 3361): accuracy: 0.9597, terra_loss: 0.1428\n",
      "06/05 08:06:55 AM: Update 3392: task terra, steps since last val 267 (total steps = 3392): accuracy: 0.9625, terra_loss: 0.1312\n",
      "06/05 08:07:06 AM: Update 3423: task terra, steps since last val 298 (total steps = 3423): accuracy: 0.9631, terra_loss: 0.1265\n",
      "06/05 08:07:16 AM: Update 3452: task terra, steps since last val 327 (total steps = 3452): accuracy: 0.9656, terra_loss: 0.1180\n",
      "06/05 08:07:26 AM: Update 3481: task terra, steps since last val 356 (total steps = 3481): accuracy: 0.9670, terra_loss: 0.1110\n",
      "06/05 08:07:36 AM: Update 3510: task terra, steps since last val 385 (total steps = 3510): accuracy: 0.9669, terra_loss: 0.1120\n",
      "06/05 08:07:46 AM: Update 3540: task terra, steps since last val 415 (total steps = 3540): accuracy: 0.9681, terra_loss: 0.1076\n",
      "06/05 08:07:56 AM: Update 3570: task terra, steps since last val 445 (total steps = 3570): accuracy: 0.9697, terra_loss: 0.1022\n",
      "06/05 08:08:06 AM: Update 3599: task terra, steps since last val 474 (total steps = 3599): accuracy: 0.9705, terra_loss: 0.0980\n",
      "06/05 08:08:16 AM: Update 3628: task terra, steps since last val 503 (total steps = 3628): accuracy: 0.9702, terra_loss: 0.0998\n",
      "06/05 08:08:27 AM: Update 3658: task terra, steps since last val 533 (total steps = 3658): accuracy: 0.9690, terra_loss: 0.1016\n",
      "06/05 08:08:37 AM: Update 3688: task terra, steps since last val 563 (total steps = 3688): accuracy: 0.9694, terra_loss: 0.1027\n",
      "06/05 08:08:47 AM: Update 3719: task terra, steps since last val 594 (total steps = 3719): accuracy: 0.9689, terra_loss: 0.1093\n",
      "06/05 08:08:57 AM: Update 3750: task terra, steps since last val 625 (total steps = 3750): accuracy: 0.9680, terra_loss: 0.1121\n",
      "06/05 08:08:57 AM: ***** Step 3750 / Validation 6 *****\n",
      "06/05 08:08:57 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:08:57 AM: Validating...\n",
      "06/05 08:09:02 AM: Updating LR scheduler:\n",
      "06/05 08:09:02 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:09:02 AM: \t# validation passes without improvement: 1\n",
      "06/05 08:09:02 AM: terra_loss: training: 0.112148 validation: 1.900102\n",
      "06/05 08:09:02 AM: macro_avg: validation: 0.631922\n",
      "06/05 08:09:02 AM: micro_avg: validation: 0.631922\n",
      "06/05 08:09:02 AM: terra_accuracy: training: 0.968000 validation: 0.631922\n",
      "06/05 08:09:02 AM: Global learning rate: 1e-05\n",
      "06/05 08:09:02 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:09:23 AM: Update 3751: task terra, steps since last val 1 (total steps = 3751): accuracy: 1.0000, terra_loss: 0.0039\n",
      "06/05 08:09:34 AM: Update 3781: task terra, steps since last val 31 (total steps = 3781): accuracy: 0.9839, terra_loss: 0.0647\n",
      "06/05 08:09:44 AM: Update 3811: task terra, steps since last val 61 (total steps = 3811): accuracy: 0.9795, terra_loss: 0.0854\n",
      "06/05 08:09:54 AM: Update 3840: task terra, steps since last val 90 (total steps = 3840): accuracy: 0.9750, terra_loss: 0.0994\n",
      "06/05 08:10:04 AM: Update 3870: task terra, steps since last val 120 (total steps = 3870): accuracy: 0.9750, terra_loss: 0.1107\n",
      "06/05 08:10:14 AM: Update 3900: task terra, steps since last val 150 (total steps = 3900): accuracy: 0.9750, terra_loss: 0.0979\n",
      "06/05 08:10:25 AM: Update 3927: task terra, steps since last val 177 (total steps = 3927): accuracy: 0.9732, terra_loss: 0.1039\n",
      "06/05 08:10:35 AM: Update 3957: task terra, steps since last val 207 (total steps = 3957): accuracy: 0.9758, terra_loss: 0.0954\n",
      "06/05 08:10:45 AM: Update 3987: task terra, steps since last val 237 (total steps = 3987): accuracy: 0.9789, terra_loss: 0.0842\n",
      "06/05 08:10:55 AM: Update 4016: task terra, steps since last val 266 (total steps = 4016): accuracy: 0.9793, terra_loss: 0.0832\n",
      "06/05 08:11:05 AM: Update 4046: task terra, steps since last val 296 (total steps = 4046): accuracy: 0.9797, terra_loss: 0.0808\n",
      "06/05 08:11:16 AM: Update 4076: task terra, steps since last val 326 (total steps = 4076): accuracy: 0.9808, terra_loss: 0.0744\n",
      "06/05 08:11:26 AM: Update 4107: task terra, steps since last val 357 (total steps = 4107): accuracy: 0.9818, terra_loss: 0.0688\n",
      "06/05 08:11:36 AM: Update 4137: task terra, steps since last val 387 (total steps = 4137): accuracy: 0.9832, terra_loss: 0.0636\n",
      "06/05 08:11:46 AM: Update 4167: task terra, steps since last val 417 (total steps = 4167): accuracy: 0.9838, terra_loss: 0.0613\n",
      "06/05 08:11:56 AM: Update 4197: task terra, steps since last val 447 (total steps = 4197): accuracy: 0.9843, terra_loss: 0.0594\n",
      "06/05 08:12:06 AM: Update 4228: task terra, steps since last val 478 (total steps = 4228): accuracy: 0.9833, terra_loss: 0.0636\n",
      "06/05 08:12:17 AM: Update 4258: task terra, steps since last val 508 (total steps = 4258): accuracy: 0.9833, terra_loss: 0.0617\n",
      "06/05 08:12:27 AM: Update 4288: task terra, steps since last val 538 (total steps = 4288): accuracy: 0.9837, terra_loss: 0.0594\n",
      "06/05 08:12:37 AM: Update 4317: task terra, steps since last val 567 (total steps = 4317): accuracy: 0.9819, terra_loss: 0.0671\n",
      "06/05 08:12:47 AM: Update 4346: task terra, steps since last val 596 (total steps = 4346): accuracy: 0.9815, terra_loss: 0.0696\n",
      "06/05 08:12:57 AM: ***** Step 4375 / Validation 7 *****\n",
      "06/05 08:12:57 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:12:57 AM: Validating...\n",
      "06/05 08:12:57 AM: Evaluate: task terra, batch 3 (77): accuracy: 0.8333, terra_loss: 1.0678\n",
      "06/05 08:13:01 AM: Updating LR scheduler:\n",
      "06/05 08:13:01 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:13:01 AM: \t# validation passes without improvement: 2\n",
      "06/05 08:13:01 AM: terra_loss: training: 0.066785 validation: 2.079546\n",
      "06/05 08:13:01 AM: macro_avg: validation: 0.641694\n",
      "06/05 08:13:01 AM: micro_avg: validation: 0.641694\n",
      "06/05 08:13:01 AM: terra_accuracy: training: 0.982400 validation: 0.641694\n",
      "06/05 08:13:01 AM: Global learning rate: 1e-05\n",
      "06/05 08:13:01 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:13:23 AM: Update 4376: task terra, steps since last val 1 (total steps = 4376): accuracy: 1.0000, terra_loss: 0.0009\n",
      "06/05 08:13:34 AM: Update 4406: task terra, steps since last val 31 (total steps = 4406): accuracy: 0.9839, terra_loss: 0.0572\n",
      "06/05 08:13:45 AM: Update 4436: task terra, steps since last val 61 (total steps = 4436): accuracy: 0.9918, terra_loss: 0.0317\n",
      "06/05 08:13:55 AM: Update 4466: task terra, steps since last val 91 (total steps = 4466): accuracy: 0.9863, terra_loss: 0.0475\n",
      "06/05 08:14:05 AM: Update 4496: task terra, steps since last val 121 (total steps = 4496): accuracy: 0.9897, terra_loss: 0.0368\n",
      "06/05 08:14:15 AM: Update 4527: task terra, steps since last val 152 (total steps = 4527): accuracy: 0.9901, terra_loss: 0.0343\n",
      "06/05 08:14:25 AM: Update 4556: task terra, steps since last val 181 (total steps = 4556): accuracy: 0.9903, terra_loss: 0.0336\n",
      "06/05 08:14:35 AM: Update 4582: task terra, steps since last val 207 (total steps = 4582): accuracy: 0.9903, terra_loss: 0.0342\n",
      "06/05 08:14:46 AM: Update 4612: task terra, steps since last val 237 (total steps = 4612): accuracy: 0.9916, terra_loss: 0.0307\n",
      "06/05 08:14:56 AM: Update 4642: task terra, steps since last val 267 (total steps = 4642): accuracy: 0.9916, terra_loss: 0.0313\n",
      "06/05 08:15:06 AM: Update 4672: task terra, steps since last val 297 (total steps = 4672): accuracy: 0.9916, terra_loss: 0.0303\n",
      "06/05 08:15:16 AM: Update 4702: task terra, steps since last val 327 (total steps = 4702): accuracy: 0.9924, terra_loss: 0.0277\n",
      "06/05 08:15:26 AM: Update 4732: task terra, steps since last val 357 (total steps = 4732): accuracy: 0.9923, terra_loss: 0.0274\n",
      "06/05 08:15:37 AM: Update 4762: task terra, steps since last val 387 (total steps = 4762): accuracy: 0.9929, terra_loss: 0.0254\n",
      "06/05 08:15:47 AM: Update 4792: task terra, steps since last val 417 (total steps = 4792): accuracy: 0.9916, terra_loss: 0.0337\n",
      "06/05 08:15:57 AM: Update 4822: task terra, steps since last val 447 (total steps = 4822): accuracy: 0.9922, terra_loss: 0.0315\n",
      "06/05 08:16:07 AM: Update 4853: task terra, steps since last val 478 (total steps = 4853): accuracy: 0.9927, terra_loss: 0.0295\n",
      "06/05 08:16:17 AM: Update 4883: task terra, steps since last val 508 (total steps = 4883): accuracy: 0.9921, terra_loss: 0.0309\n",
      "06/05 08:16:27 AM: Update 4913: task terra, steps since last val 538 (total steps = 4913): accuracy: 0.9926, terra_loss: 0.0295\n",
      "06/05 08:16:38 AM: Update 4943: task terra, steps since last val 568 (total steps = 4943): accuracy: 0.9925, terra_loss: 0.0284\n",
      "06/05 08:16:48 AM: Update 4972: task terra, steps since last val 597 (total steps = 4972): accuracy: 0.9920, terra_loss: 0.0300\n",
      "06/05 08:16:57 AM: ***** Step 5000 / Validation 8 *****\n",
      "06/05 08:16:57 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:16:57 AM: Validating...\n",
      "06/05 08:16:58 AM: Evaluate: task terra, batch 14 (77): accuracy: 0.6071, terra_loss: 2.3298\n",
      "06/05 08:17:02 AM: Updating LR scheduler:\n",
      "06/05 08:17:02 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:17:02 AM: \t# validation passes without improvement: 3\n",
      "06/05 08:17:02 AM: terra_loss: training: 0.029260 validation: 2.281839\n",
      "06/05 08:17:02 AM: macro_avg: validation: 0.635179\n",
      "06/05 08:17:02 AM: micro_avg: validation: 0.635179\n",
      "06/05 08:17:02 AM: terra_accuracy: training: 0.992000 validation: 0.635179\n",
      "06/05 08:17:02 AM: Global learning rate: 1e-05\n",
      "06/05 08:17:02 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:17:23 AM: Update 5001: task terra, steps since last val 1 (total steps = 5001): accuracy: 1.0000, terra_loss: 0.0001\n",
      "06/05 08:17:33 AM: Update 5029: task terra, steps since last val 29 (total steps = 5029): accuracy: 0.9828, terra_loss: 0.1186\n",
      "06/05 08:17:43 AM: Update 5059: task terra, steps since last val 59 (total steps = 5059): accuracy: 0.9873, terra_loss: 0.0681\n",
      "06/05 08:17:54 AM: Update 5090: task terra, steps since last val 90 (total steps = 5090): accuracy: 0.9917, terra_loss: 0.0450\n",
      "06/05 08:18:04 AM: Update 5121: task terra, steps since last val 121 (total steps = 5121): accuracy: 0.9938, terra_loss: 0.0339\n",
      "06/05 08:18:14 AM: Update 5151: task terra, steps since last val 151 (total steps = 5151): accuracy: 0.9917, terra_loss: 0.0479\n",
      "06/05 08:18:24 AM: Update 5182: task terra, steps since last val 182 (total steps = 5182): accuracy: 0.9918, terra_loss: 0.0448\n",
      "06/05 08:18:34 AM: Update 5212: task terra, steps since last val 212 (total steps = 5212): accuracy: 0.9917, terra_loss: 0.0440\n",
      "06/05 08:18:45 AM: Update 5240: task terra, steps since last val 240 (total steps = 5240): accuracy: 0.9927, terra_loss: 0.0393\n",
      "06/05 08:18:55 AM: Update 5269: task terra, steps since last val 269 (total steps = 5269): accuracy: 0.9926, terra_loss: 0.0386\n",
      "06/05 08:19:05 AM: Update 5299: task terra, steps since last val 299 (total steps = 5299): accuracy: 0.9933, terra_loss: 0.0352\n",
      "06/05 08:19:15 AM: Update 5330: task terra, steps since last val 330 (total steps = 5330): accuracy: 0.9939, terra_loss: 0.0319\n",
      "06/05 08:19:26 AM: Update 5361: task terra, steps since last val 361 (total steps = 5361): accuracy: 0.9938, terra_loss: 0.0320\n",
      "06/05 08:19:36 AM: Update 5392: task terra, steps since last val 392 (total steps = 5392): accuracy: 0.9936, terra_loss: 0.0310\n",
      "06/05 08:19:46 AM: Update 5421: task terra, steps since last val 421 (total steps = 5421): accuracy: 0.9941, terra_loss: 0.0289\n",
      "06/05 08:19:56 AM: Update 5451: task terra, steps since last val 451 (total steps = 5451): accuracy: 0.9945, terra_loss: 0.0270\n",
      "06/05 08:20:06 AM: Update 5481: task terra, steps since last val 481 (total steps = 5481): accuracy: 0.9948, terra_loss: 0.0255\n",
      "06/05 08:20:16 AM: Update 5511: task terra, steps since last val 511 (total steps = 5511): accuracy: 0.9951, terra_loss: 0.0240\n",
      "06/05 08:20:26 AM: Update 5540: task terra, steps since last val 540 (total steps = 5540): accuracy: 0.9949, terra_loss: 0.0232\n",
      "06/05 08:20:36 AM: Update 5570: task terra, steps since last val 570 (total steps = 5570): accuracy: 0.9952, terra_loss: 0.0221\n",
      "06/05 08:20:47 AM: Update 5601: task terra, steps since last val 601 (total steps = 5601): accuracy: 0.9946, terra_loss: 0.0253\n",
      "06/05 08:20:55 AM: ***** Step 5625 / Validation 9 *****\n",
      "06/05 08:20:55 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:20:55 AM: Validating...\n",
      "06/05 08:20:57 AM: Evaluate: task terra, batch 29 (77): accuracy: 0.6638, terra_loss: 2.3476\n",
      "06/05 08:21:00 AM: Updating LR scheduler:\n",
      "06/05 08:21:00 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:21:00 AM: \t# validation passes without improvement: 4\n",
      "06/05 08:21:00 AM: terra_loss: training: 0.024411 validation: 2.394197\n",
      "06/05 08:21:00 AM: macro_avg: validation: 0.651466\n",
      "06/05 08:21:00 AM: micro_avg: validation: 0.651466\n",
      "06/05 08:21:00 AM: terra_accuracy: training: 0.994800 validation: 0.651466\n",
      "06/05 08:21:00 AM: Global learning rate: 1e-05\n",
      "06/05 08:21:00 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:21:20 AM: Update 5626: task terra, steps since last val 1 (total steps = 5626): accuracy: 1.0000, terra_loss: 0.0001\n",
      "06/05 08:21:31 AM: Update 5656: task terra, steps since last val 31 (total steps = 5656): accuracy: 1.0000, terra_loss: 0.0019\n",
      "06/05 08:21:41 AM: Update 5686: task terra, steps since last val 61 (total steps = 5686): accuracy: 0.9959, terra_loss: 0.0299\n",
      "06/05 08:21:51 AM: Update 5716: task terra, steps since last val 91 (total steps = 5716): accuracy: 0.9918, terra_loss: 0.0448\n",
      "06/05 08:22:02 AM: Update 5746: task terra, steps since last val 121 (total steps = 5746): accuracy: 0.9917, terra_loss: 0.0484\n",
      "06/05 08:22:12 AM: Update 5775: task terra, steps since last val 150 (total steps = 5775): accuracy: 0.9917, terra_loss: 0.0418\n",
      "06/05 08:22:22 AM: Update 5804: task terra, steps since last val 179 (total steps = 5804): accuracy: 0.9916, terra_loss: 0.0364\n",
      "06/05 08:22:32 AM: Update 5835: task terra, steps since last val 210 (total steps = 5835): accuracy: 0.9929, terra_loss: 0.0313\n",
      "06/05 08:22:42 AM: Update 5866: task terra, steps since last val 241 (total steps = 5866): accuracy: 0.9927, terra_loss: 0.0308\n",
      "06/05 08:22:52 AM: Update 5894: task terra, steps since last val 269 (total steps = 5894): accuracy: 0.9926, terra_loss: 0.0343\n",
      "06/05 08:23:03 AM: Update 5925: task terra, steps since last val 300 (total steps = 5925): accuracy: 0.9933, terra_loss: 0.0309\n",
      "06/05 08:23:13 AM: Update 5955: task terra, steps since last val 330 (total steps = 5955): accuracy: 0.9939, terra_loss: 0.0281\n",
      "06/05 08:23:23 AM: Update 5984: task terra, steps since last val 359 (total steps = 5984): accuracy: 0.9944, terra_loss: 0.0259\n",
      "06/05 08:23:33 AM: Update 6014: task terra, steps since last val 389 (total steps = 6014): accuracy: 0.9949, terra_loss: 0.0239\n",
      "06/05 08:23:43 AM: Update 6043: task terra, steps since last val 418 (total steps = 6043): accuracy: 0.9946, terra_loss: 0.0232\n",
      "06/05 08:23:53 AM: Update 6073: task terra, steps since last val 448 (total steps = 6073): accuracy: 0.9950, terra_loss: 0.0216\n",
      "06/05 08:24:04 AM: Update 6104: task terra, steps since last val 479 (total steps = 6104): accuracy: 0.9948, terra_loss: 0.0207\n",
      "06/05 08:24:14 AM: Update 6134: task terra, steps since last val 509 (total steps = 6134): accuracy: 0.9946, terra_loss: 0.0200\n",
      "06/05 08:24:24 AM: Update 6165: task terra, steps since last val 540 (total steps = 6165): accuracy: 0.9949, terra_loss: 0.0190\n",
      "06/05 08:24:34 AM: Update 6196: task terra, steps since last val 571 (total steps = 6196): accuracy: 0.9947, terra_loss: 0.0187\n",
      "06/05 08:24:45 AM: Update 6226: task terra, steps since last val 601 (total steps = 6226): accuracy: 0.9946, terra_loss: 0.0187\n",
      "06/05 08:24:53 AM: ***** Step 6250 / Validation 10 *****\n",
      "06/05 08:24:53 AM: terra: trained on 625 steps (625 batches) since val, 0.956 epochs\n",
      "06/05 08:24:53 AM: Validating...\n",
      "06/05 08:24:55 AM: Evaluate: task terra, batch 30 (77): accuracy: 0.6833, terra_loss: 2.2974\n",
      "06/05 08:24:57 AM: Updating LR scheduler:\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-06.\n",
      "06/05 08:24:57 AM: \tBest result seen so far for macro_avg: 0.655\n",
      "06/05 08:24:57 AM: \t# validation passes without improvement: 0\n",
      "06/05 08:24:57 AM: Maximum number of validations reached. Stopping training.\n",
      "06/05 08:24:57 AM: terra_loss: training: 0.019594 validation: 2.425766\n",
      "06/05 08:24:57 AM: macro_avg: validation: 0.651466\n",
      "06/05 08:24:57 AM: micro_avg: validation: 0.651466\n",
      "06/05 08:24:57 AM: terra_accuracy: training: 0.994400 validation: 0.651466\n",
      "06/05 08:24:57 AM: Global learning rate: 5e-06\n",
      "06/05 08:24:57 AM: Saving checkpoints to: ./model_dir//rubert/terra\n",
      "06/05 08:25:18 AM: Stopped training after 10 validation checks\n",
      "06/05 08:25:21 AM: Trained terra for 6250 steps or 9.557 epochs\n",
      "06/05 08:25:21 AM: ***** VALIDATION RESULTS *****\n",
      "06/05 08:25:21 AM: terra_accuracy (for best val pass 5): terra_loss: 1.65142, macro_avg: 0.65472, micro_avg: 0.65472, terra_accuracy: 0.65472\n",
      "06/05 08:25:21 AM: micro_avg (for best val pass 5): terra_loss: 1.65142, macro_avg: 0.65472, micro_avg: 0.65472, terra_accuracy: 0.65472\n",
      "06/05 08:25:21 AM: macro_avg (for best val pass 5): terra_loss: 1.65142, macro_avg: 0.65472, micro_avg: 0.65472, terra_accuracy: 0.65472\n",
      "06/05 08:25:21 AM: Evaluating...\n",
      "06/05 08:25:42 AM: Loaded model state from ./model_dir//rubert/terra/model_state_pretrain_val_5.best.th\n",
      "06/05 08:25:42 AM: Evaluating on: lidirus, split: val\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "06/05 08:25:56 AM: Task 'lidirus': sorting predictions by 'idx'\n",
      "06/05 08:25:56 AM: Finished evaluating on: lidirus\n",
      "06/05 08:25:56 AM: Task 'lidirus': Wrote predictions to ./model_dir//rubert/terra\n",
      "06/05 08:25:56 AM: Wrote all preds for split 'val' to ./model_dir//rubert/terra\n",
      "06/05 08:25:56 AM: Evaluating on: lidirus, split: test\n",
      "06/05 08:26:10 AM: Task 'lidirus': sorting predictions by 'idx'\n",
      "06/05 08:26:10 AM: Finished evaluating on: lidirus\n",
      "06/05 08:26:10 AM: Task 'lidirus': Wrote predictions to ./model_dir//rubert/terra\n",
      "06/05 08:26:10 AM: Wrote all preds for split 'test' to ./model_dir//rubert/terra\n",
      "06/05 08:26:10 AM: Writing results for split 'val' to ./model_dir//rubert/results.tsv\n",
      "06/05 08:26:10 AM: micro_avg: 0.000, macro_avg: 0.000, lidirus_lex_sem: -0.051, lidirus_lex_sem__Factivity: -0.034, lidirus_lex_sem__Factivity;Quantifiers: 0.000, lidirus_lex_sem__Quantifiers: -0.166, lidirus_lex_sem__Named entities: -0.302, lidirus_lex_sem__Lexical entailment;Factivity: 0.000, lidirus_lex_sem__Morphological negation: -0.312, lidirus_lex_sem__Lexical entailment;Quantifiers: 0.000, lidirus_lex_sem__Redundancy: 0.060, lidirus_lex_sem__Lexical entailment: 0.023, lidirus_pr_ar_str: -0.053, lidirus_pr_ar_str__Genitives/Partitives: 0.000, lidirus_pr_ar_str__Coordination scope;Prepositional phrases: 0.000, lidirus_pr_ar_str__Intersectivity: 0.044, lidirus_pr_ar_str__Anaphora/Coreference;Prepositional phrases: 0.000, lidirus_pr_ar_str__Restrictivity;Anaphora/Coreference: 0.000, lidirus_pr_ar_str__Active/Passive: -0.158, lidirus_pr_ar_str__Active/Passive;Prepositional phrases: -1.000, lidirus_pr_ar_str__Intersectivity;Ellipsis/Implicits: 0.000, lidirus_pr_ar_str__Relative clauses;Restrictivity: 0.000, lidirus_pr_ar_str__Nominalization;Genitives/Partitives: 0.000, lidirus_pr_ar_str__Nominalization: 0.000, lidirus_pr_ar_str__Restrictivity: 0.000, lidirus_pr_ar_str__Anaphora/Coreference: -0.123, lidirus_pr_ar_str__Restrictivity;Relative clauses: 0.000, lidirus_pr_ar_str__Ellipsis/Implicits: 0.000, lidirus_pr_ar_str__Core args: 0.238, lidirus_pr_ar_str__Coordination scope: 0.000, lidirus_pr_ar_str__Core args;Anaphora/Coreference: 0.000, lidirus_pr_ar_str__Relative clauses;Anaphora/Coreference: 0.000, lidirus_pr_ar_str__Datives: -0.218, lidirus_pr_ar_str__Prepositional phrases: -0.302, lidirus_pr_ar_str__Ellipsis/Implicits;Anaphora/Coreference: 0.000, lidirus_logic: -0.078, lidirus_logic__Existential: -0.389, lidirus_logic__Universal;Conjunction: 0.000, lidirus_logic__Temporal: -0.223, lidirus_logic__Negation: -0.077, lidirus_logic__Conditionals: 0.000, lidirus_logic__Intervals/Numbers: -0.102, lidirus_logic__Double negation;Negation: 0.000, lidirus_logic__Universal;Negation: 0.000, lidirus_logic__Negation;Conditionals: 0.000, lidirus_logic__Non-monotone: 0.000, lidirus_logic__Disjunction;Non-monotone: 0.000, lidirus_logic__Downward monotone;Existential;Negation: 0.000, lidirus_logic__Intervals/Numbers;Non-monotone: 0.000, lidirus_logic__Temporal;Conjunction: 0.000, lidirus_logic__Disjunction;Conjunction: 0.000, lidirus_logic__Disjunction;Conditionals;Negation: -0.577, lidirus_logic__Downward monotone;Conditionals: 0.000, lidirus_logic__Upward monotone: 0.150, lidirus_logic__Conjunction;Upward monotone: 0.000, lidirus_logic__Universal: -0.026, lidirus_logic__Existential;Upward monotone: 0.000, lidirus_logic__Existential;Negation: 0.000, lidirus_logic__Downward monotone: 0.045, lidirus_logic__Conjunction: 0.000, lidirus_logic__Temporal;Intervals/Numbers: 0.000, lidirus_logic__Disjunction;Negation: -0.200, lidirus_logic__Double negation: -0.469, lidirus_logic__Conjunction;Negation: 0.000, lidirus_knowledge: -0.041, lidirus_knowledge__World knowledge: -0.090, lidirus_all_mcc: -0.062, lidirus_accuracy: 0.555\n",
      "06/05 08:26:10 AM: Loaded model state from ./model_dir//rubert/terra/model_state_pretrain_val_5.best.th\n",
      "06/05 08:26:10 AM: Evaluating on: terra, split: val\n",
      "06/05 08:26:15 AM: Task 'terra': sorting predictions by 'idx'\n",
      "06/05 08:26:15 AM: Finished evaluating on: terra\n",
      "06/05 08:26:15 AM: Task 'terra': Wrote predictions to ./model_dir//rubert/terra\n",
      "06/05 08:26:15 AM: Wrote all preds for split 'val' to ./model_dir//rubert/terra\n",
      "06/05 08:26:15 AM: Evaluating on: terra, split: test\n",
      "06/05 08:26:45 AM: \tTask terra: batch 488\n",
      "06/05 08:27:05 AM: Task 'terra': sorting predictions by 'idx'\n",
      "06/05 08:27:05 AM: Finished evaluating on: terra\n",
      "06/05 08:27:05 AM: Task 'terra': Wrote predictions to ./model_dir//rubert/terra\n",
      "06/05 08:27:05 AM: Wrote all preds for split 'test' to ./model_dir//rubert/terra\n",
      "06/05 08:27:05 AM: Writing results for split 'val' to ./model_dir//rubert/results.tsv\n",
      "06/05 08:27:05 AM: micro_avg: 0.655, macro_avg: 0.655, terra_accuracy: 0.655\n",
      "06/05 08:27:05 AM: Done!\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 ./scripts/russian-superglue-baselines.sh \n",
    "! ./scripts/russian-superglue-baselines.sh \"terra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDEdbRNybYB6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Russian SuperGLUE example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
